{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IZP5-IUDsEf7"
      },
      "id": "IZP5-IUDsEf7"
    },
    {
      "cell_type": "markdown",
      "id": "e5caa54d",
      "metadata": {
        "id": "e5caa54d"
      },
      "source": [
        "## 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2996d991",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2996d991",
        "outputId": "6bf1d692-3803-4f80-f00d-7c43979f9cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "\n",
        "# model for forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "    print(\"w=\", w)\n",
        "    l_sum = 0\n",
        "\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val, y_val)\n",
        "        l_sum += l\n",
        "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "\n",
        "    print(\"MSE=\", l_sum / len(x_data))\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / len(x_data))\n",
        "\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "219650d4",
      "metadata": {
        "id": "219650d4"
      },
      "source": [
        "## 2- manual gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d9d187d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9d187d7",
        "outputId": "155ff942-07e6-4742-f7ee-9afab47a6780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad:  1.0 2.0 -1.48\n",
            "\tgrad:  2.0 4.0 -5.8\n",
            "\tgrad:  3.0 6.0 -12.0\n",
            "progress: 1 w= 1.45 loss= 2.69\n",
            "\tgrad:  1.0 2.0 -1.09\n",
            "\tgrad:  2.0 4.0 -4.29\n",
            "\tgrad:  3.0 6.0 -8.87\n",
            "progress: 2 w= 1.6 loss= 1.47\n",
            "\tgrad:  1.0 2.0 -0.81\n",
            "\tgrad:  2.0 4.0 -3.17\n",
            "\tgrad:  3.0 6.0 -6.56\n",
            "progress: 3 w= 1.7 loss= 0.8\n",
            "\tgrad:  1.0 2.0 -0.6\n",
            "\tgrad:  2.0 4.0 -2.34\n",
            "\tgrad:  3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad:  1.0 2.0 -0.44\n",
            "\tgrad:  2.0 4.0 -1.73\n",
            "\tgrad:  3.0 6.0 -3.58\n",
            "progress: 5 w= 1.84 loss= 0.24\n",
            "\tgrad:  1.0 2.0 -0.33\n",
            "\tgrad:  2.0 4.0 -1.28\n",
            "\tgrad:  3.0 6.0 -2.65\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad:  1.0 2.0 -0.24\n",
            "\tgrad:  2.0 4.0 -0.95\n",
            "\tgrad:  3.0 6.0 -1.96\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad:  1.0 2.0 -0.18\n",
            "\tgrad:  2.0 4.0 -0.7\n",
            "\tgrad:  3.0 6.0 -1.45\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad:  1.0 2.0 -0.13\n",
            "\tgrad:  2.0 4.0 -0.52\n",
            "\tgrad:  3.0 6.0 -1.07\n",
            "progress: 9 w= 1.95 loss= 0.02\n",
            "Predicted score after trainin) 4 hours of studying: 7.804863933862125\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "w = 1.0  #random\n",
        "\n",
        "\n",
        "def gradient(x, y):\n",
        "    return 2 * x * (x * w - y)\n",
        "\n",
        "\n",
        "print(\"Prediction before training\",  4, forward(4))\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "\n",
        "        grad = gradient(x_val, y_val)\n",
        "        w = w - 0.01 * grad\n",
        "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
        "        l = loss(x_val, y_val)\n",
        "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
        "\n",
        "# After training\n",
        "print(\"Predicted score after trainin)\",  \"4 hours of studying:\", forward(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42cd3384",
      "metadata": {
        "id": "42cd3384"
      },
      "source": [
        "## 3 - auto gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4fd31831",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fd31831",
        "outputId": "b86efa8c-afa6-4f76-9775-324896b4237c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training 4 4.0\n",
            "\t grad:  1.0 2.0 -2.0\n",
            "\t grad:  2.0 4.0 -7.840000152587891\n",
            "\t grad:  3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 | Loss: 7.315943717956543\n",
            "\t grad:  1.0 2.0 -1.478623867034912\n",
            "\t grad:  2.0 4.0 -5.796205520629883\n",
            "\t grad:  3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 | Loss: 3.9987640380859375\n",
            "\t grad:  1.0 2.0 -1.0931644439697266\n",
            "\t grad:  2.0 4.0 -4.285204887390137\n",
            "\t grad:  3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 | Loss: 2.1856532096862793\n",
            "\t grad:  1.0 2.0 -0.8081896305084229\n",
            "\t grad:  2.0 4.0 -3.1681032180786133\n",
            "\t grad:  3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 | Loss: 1.1946394443511963\n",
            "\t grad:  1.0 2.0 -0.5975041389465332\n",
            "\t grad:  2.0 4.0 -2.3422164916992188\n",
            "\t grad:  3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 | Loss: 0.6529689431190491\n",
            "\t grad:  1.0 2.0 -0.4417421817779541\n",
            "\t grad:  2.0 4.0 -1.7316293716430664\n",
            "\t grad:  3.0 6.0 -3.58447265625\n",
            "Epoch: 5 | Loss: 0.35690122842788696\n",
            "\t grad:  1.0 2.0 -0.3265852928161621\n",
            "\t grad:  2.0 4.0 -1.2802143096923828\n",
            "\t grad:  3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 | Loss: 0.195076122879982\n",
            "\t grad:  1.0 2.0 -0.24144840240478516\n",
            "\t grad:  2.0 4.0 -0.9464778900146484\n",
            "\t grad:  3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 | Loss: 0.10662525147199631\n",
            "\t grad:  1.0 2.0 -0.17850565910339355\n",
            "\t grad:  2.0 4.0 -0.699742317199707\n",
            "\t grad:  3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 | Loss: 0.0582793727517128\n",
            "\t grad:  1.0 2.0 -0.1319713592529297\n",
            "\t grad:  2.0 4.0 -0.5173273086547852\n",
            "\t grad:  3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 | Loss: 0.03185431286692619\n",
            "Prediction after training 4 7.804864406585693\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pdb\n",
        "\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "\n",
        "# new loss function\n",
        "def loss(y_pred, y_val):\n",
        "    return (y_pred - y_val) ** 2\n",
        "\n",
        "print(\"Prediction before training\",  4, forward(4).item())\n",
        "\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred = forward(x_val) #forward pass\n",
        "        l = loss(y_pred, y_val) # compute loss\n",
        "        l.backward() #backpropagation to update weights\n",
        "        print(\"\\t grad: \", x_val, y_val, w.grad.item())\n",
        "        w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "        w.grad.data.zero_()\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
        "\n",
        "print(\"Prediction after training\",  4, forward(4).item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1fb4c89",
      "metadata": {
        "id": "a1fb4c89"
      },
      "source": [
        "## 5 -  linear_regression\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d4fe0acf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4fe0acf",
        "outputId": "14a4ffd5-3279-489f-e5eb-ef2df3e9bcd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 103.82003784179688 \n",
            "Epoch: 1 | Loss: 46.24197769165039 \n",
            "Epoch: 2 | Loss: 20.609508514404297 \n",
            "Epoch: 3 | Loss: 9.198319435119629 \n",
            "Epoch: 4 | Loss: 4.1180419921875 \n",
            "Epoch: 5 | Loss: 1.8561152219772339 \n",
            "Epoch: 6 | Loss: 0.848840594291687 \n",
            "Epoch: 7 | Loss: 0.4001062512397766 \n",
            "Epoch: 8 | Loss: 0.20002268254756927 \n",
            "Epoch: 9 | Loss: 0.11063649505376816 \n",
            "Epoch: 10 | Loss: 0.07053400576114655 \n",
            "Epoch: 11 | Loss: 0.05237545818090439 \n",
            "Epoch: 12 | Loss: 0.04399051517248154 \n",
            "Epoch: 13 | Loss: 0.03996068984270096 \n",
            "Epoch: 14 | Loss: 0.037873826920986176 \n",
            "Epoch: 15 | Loss: 0.03665607422590256 \n",
            "Epoch: 16 | Loss: 0.03582953289151192 \n",
            "Epoch: 17 | Loss: 0.03518117591738701 \n",
            "Epoch: 18 | Loss: 0.03461620584130287 \n",
            "Epoch: 19 | Loss: 0.034092314541339874 \n",
            "Epoch: 20 | Loss: 0.033590562641620636 \n",
            "Epoch: 21 | Loss: 0.03310251235961914 \n",
            "Epoch: 22 | Loss: 0.0326244980096817 \n",
            "Epoch: 23 | Loss: 0.032154541462659836 \n",
            "Epoch: 24 | Loss: 0.031692009419202805 \n",
            "Epoch: 25 | Loss: 0.031236328184604645 \n",
            "Epoch: 26 | Loss: 0.030787335708737373 \n",
            "Epoch: 27 | Loss: 0.03034481406211853 \n",
            "Epoch: 28 | Loss: 0.029908591881394386 \n",
            "Epoch: 29 | Loss: 0.02947886846959591 \n",
            "Epoch: 30 | Loss: 0.029055248945951462 \n",
            "Epoch: 31 | Loss: 0.028637636452913284 \n",
            "Epoch: 32 | Loss: 0.028226057067513466 \n",
            "Epoch: 33 | Loss: 0.027820277959108353 \n",
            "Epoch: 34 | Loss: 0.02742045186460018 \n",
            "Epoch: 35 | Loss: 0.027026470750570297 \n",
            "Epoch: 36 | Loss: 0.02663804590702057 \n",
            "Epoch: 37 | Loss: 0.026255182921886444 \n",
            "Epoch: 38 | Loss: 0.02587793581187725 \n",
            "Epoch: 39 | Loss: 0.025505980476737022 \n",
            "Epoch: 40 | Loss: 0.025139465928077698 \n",
            "Epoch: 41 | Loss: 0.024778129532933235 \n",
            "Epoch: 42 | Loss: 0.024421988055109978 \n",
            "Epoch: 43 | Loss: 0.024070996791124344 \n",
            "Epoch: 44 | Loss: 0.02372507005929947 \n",
            "Epoch: 45 | Loss: 0.023384112864732742 \n",
            "Epoch: 46 | Loss: 0.02304806560277939 \n",
            "Epoch: 47 | Loss: 0.022716861218214035 \n",
            "Epoch: 48 | Loss: 0.022390354424715042 \n",
            "Epoch: 49 | Loss: 0.022068513557314873 \n",
            "Epoch: 50 | Loss: 0.021751463413238525 \n",
            "Epoch: 51 | Loss: 0.0214388445019722 \n",
            "Epoch: 52 | Loss: 0.021130695939064026 \n",
            "Epoch: 53 | Loss: 0.020827068015933037 \n",
            "Epoch: 54 | Loss: 0.020527727901935577 \n",
            "Epoch: 55 | Loss: 0.020232640206813812 \n",
            "Epoch: 56 | Loss: 0.019941920414566994 \n",
            "Epoch: 57 | Loss: 0.019655318930745125 \n",
            "Epoch: 58 | Loss: 0.0193728469312191 \n",
            "Epoch: 59 | Loss: 0.019094452261924744 \n",
            "Epoch: 60 | Loss: 0.018820036202669144 \n",
            "Epoch: 61 | Loss: 0.01854950562119484 \n",
            "Epoch: 62 | Loss: 0.018282946199178696 \n",
            "Epoch: 63 | Loss: 0.01802022010087967 \n",
            "Epoch: 64 | Loss: 0.01776118576526642 \n",
            "Epoch: 65 | Loss: 0.017506003379821777 \n",
            "Epoch: 66 | Loss: 0.017254415899515152 \n",
            "Epoch: 67 | Loss: 0.017006410285830498 \n",
            "Epoch: 68 | Loss: 0.016762012615799904 \n",
            "Epoch: 69 | Loss: 0.01652110554277897 \n",
            "Epoch: 70 | Loss: 0.01628364622592926 \n",
            "Epoch: 71 | Loss: 0.016049638390541077 \n",
            "Epoch: 72 | Loss: 0.015819011256098747 \n",
            "Epoch: 73 | Loss: 0.015591608360409737 \n",
            "Epoch: 74 | Loss: 0.015367609448730946 \n",
            "Epoch: 75 | Loss: 0.015146699734032154 \n",
            "Epoch: 76 | Loss: 0.014928988181054592 \n",
            "Epoch: 77 | Loss: 0.014714431017637253 \n",
            "Epoch: 78 | Loss: 0.014503003098070621 \n",
            "Epoch: 79 | Loss: 0.014294557273387909 \n",
            "Epoch: 80 | Loss: 0.01408911868929863 \n",
            "Epoch: 81 | Loss: 0.013886653818190098 \n",
            "Epoch: 82 | Loss: 0.013687082566320896 \n",
            "Epoch: 83 | Loss: 0.01349041610956192 \n",
            "Epoch: 84 | Loss: 0.013296517543494701 \n",
            "Epoch: 85 | Loss: 0.0131054213270545 \n",
            "Epoch: 86 | Loss: 0.01291702501475811 \n",
            "Epoch: 87 | Loss: 0.012731427326798439 \n",
            "Epoch: 88 | Loss: 0.012548437342047691 \n",
            "Epoch: 89 | Loss: 0.012368084862828255 \n",
            "Epoch: 90 | Loss: 0.012190372683107853 \n",
            "Epoch: 91 | Loss: 0.012015189975500107 \n",
            "Epoch: 92 | Loss: 0.011842489242553711 \n",
            "Epoch: 93 | Loss: 0.011672314256429672 \n",
            "Epoch: 94 | Loss: 0.01150454394519329 \n",
            "Epoch: 95 | Loss: 0.011339236982166767 \n",
            "Epoch: 96 | Loss: 0.011176271364092827 \n",
            "Epoch: 97 | Loss: 0.011015619151294231 \n",
            "Epoch: 98 | Loss: 0.010857325047254562 \n",
            "Epoch: 99 | Loss: 0.010701281949877739 \n",
            "Epoch: 100 | Loss: 0.010547485202550888 \n",
            "Epoch: 101 | Loss: 0.01039591059088707 \n",
            "Epoch: 102 | Loss: 0.01024648081511259 \n",
            "Epoch: 103 | Loss: 0.010099242441356182 \n",
            "Epoch: 104 | Loss: 0.009954067878425121 \n",
            "Epoch: 105 | Loss: 0.009811043739318848 \n",
            "Epoch: 106 | Loss: 0.009670084342360497 \n",
            "Epoch: 107 | Loss: 0.00953108724206686 \n",
            "Epoch: 108 | Loss: 0.009394071064889431 \n",
            "Epoch: 109 | Loss: 0.009259075857698917 \n",
            "Epoch: 110 | Loss: 0.009125986136496067 \n",
            "Epoch: 111 | Loss: 0.008994879201054573 \n",
            "Epoch: 112 | Loss: 0.00886557623744011 \n",
            "Epoch: 113 | Loss: 0.008738191798329353 \n",
            "Epoch: 114 | Loss: 0.008612560108304024 \n",
            "Epoch: 115 | Loss: 0.008488830178976059 \n",
            "Epoch: 116 | Loss: 0.00836684089154005 \n",
            "Epoch: 117 | Loss: 0.008246579207479954 \n",
            "Epoch: 118 | Loss: 0.008128095418214798 \n",
            "Epoch: 119 | Loss: 0.008011233061552048 \n",
            "Epoch: 120 | Loss: 0.00789614673703909 \n",
            "Epoch: 121 | Loss: 0.007782647851854563 \n",
            "Epoch: 122 | Loss: 0.007670785300433636 \n",
            "Epoch: 123 | Loss: 0.0075605567544698715 \n",
            "Epoch: 124 | Loss: 0.007451878394931555 \n",
            "Epoch: 125 | Loss: 0.0073447697795927525 \n",
            "Epoch: 126 | Loss: 0.007239207625389099 \n",
            "Epoch: 127 | Loss: 0.007135195191949606 \n",
            "Epoch: 128 | Loss: 0.007032653316855431 \n",
            "Epoch: 129 | Loss: 0.006931569427251816 \n",
            "Epoch: 130 | Loss: 0.00683199055492878 \n",
            "Epoch: 131 | Loss: 0.0067337858490645885 \n",
            "Epoch: 132 | Loss: 0.006636979524046183 \n",
            "Epoch: 133 | Loss: 0.0065416451543569565 \n",
            "Epoch: 134 | Loss: 0.006447593681514263 \n",
            "Epoch: 135 | Loss: 0.00635492242872715 \n",
            "Epoch: 136 | Loss: 0.006263599265366793 \n",
            "Epoch: 137 | Loss: 0.0061735850758850574 \n",
            "Epoch: 138 | Loss: 0.0060848756693303585 \n",
            "Epoch: 139 | Loss: 0.0059974342584609985 \n",
            "Epoch: 140 | Loss: 0.005911207292228937 \n",
            "Epoch: 141 | Loss: 0.005826260894536972 \n",
            "Epoch: 142 | Loss: 0.005742520093917847 \n",
            "Epoch: 143 | Loss: 0.005660010501742363 \n",
            "Epoch: 144 | Loss: 0.005578688345849514 \n",
            "Epoch: 145 | Loss: 0.005498494487255812 \n",
            "Epoch: 146 | Loss: 0.005419450346380472 \n",
            "Epoch: 147 | Loss: 0.005341590382158756 \n",
            "Epoch: 148 | Loss: 0.005264803767204285 \n",
            "Epoch: 149 | Loss: 0.005189159885048866 \n",
            "Epoch: 150 | Loss: 0.0051145912148058414 \n",
            "Epoch: 151 | Loss: 0.005041090771555901 \n",
            "Epoch: 152 | Loss: 0.004968614317476749 \n",
            "Epoch: 153 | Loss: 0.0048972321674227715 \n",
            "Epoch: 154 | Loss: 0.004826851189136505 \n",
            "Epoch: 155 | Loss: 0.004757464863359928 \n",
            "Epoch: 156 | Loss: 0.0046891141682863235 \n",
            "Epoch: 157 | Loss: 0.0046216933988034725 \n",
            "Epoch: 158 | Loss: 0.004555302206426859 \n",
            "Epoch: 159 | Loss: 0.004489819519221783 \n",
            "Epoch: 160 | Loss: 0.004425302613526583 \n",
            "Epoch: 161 | Loss: 0.0043616825714707375 \n",
            "Epoch: 162 | Loss: 0.004299011081457138 \n",
            "Epoch: 163 | Loss: 0.0042372094467282295 \n",
            "Epoch: 164 | Loss: 0.004176306538283825 \n",
            "Epoch: 165 | Loss: 0.004116294905543327 \n",
            "Epoch: 166 | Loss: 0.0040571666322648525 \n",
            "Epoch: 167 | Loss: 0.003998866770416498 \n",
            "Epoch: 168 | Loss: 0.0039413790218532085 \n",
            "Epoch: 169 | Loss: 0.003884735284373164 \n",
            "Epoch: 170 | Loss: 0.003828904591500759 \n",
            "Epoch: 171 | Loss: 0.0037738566752523184 \n",
            "Epoch: 172 | Loss: 0.0037196469493210316 \n",
            "Epoch: 173 | Loss: 0.0036661778576672077 \n",
            "Epoch: 174 | Loss: 0.0036134873516857624 \n",
            "Epoch: 175 | Loss: 0.003561546327546239 \n",
            "Epoch: 176 | Loss: 0.0035103701520711184 \n",
            "Epoch: 177 | Loss: 0.0034599066711962223 \n",
            "Epoch: 178 | Loss: 0.0034101989585906267 \n",
            "Epoch: 179 | Loss: 0.0033612013794481754 \n",
            "Epoch: 180 | Loss: 0.003312889952212572 \n",
            "Epoch: 181 | Loss: 0.003265266539528966 \n",
            "Epoch: 182 | Loss: 0.0032183413859456778 \n",
            "Epoch: 183 | Loss: 0.0031720963306725025 \n",
            "Epoch: 184 | Loss: 0.0031265239231288433 \n",
            "Epoch: 185 | Loss: 0.003081594593822956 \n",
            "Epoch: 186 | Loss: 0.0030373011250048876 \n",
            "Epoch: 187 | Loss: 0.0029936337377876043 \n",
            "Epoch: 188 | Loss: 0.0029506233986467123 \n",
            "Epoch: 189 | Loss: 0.0029082256369292736 \n",
            "Epoch: 190 | Loss: 0.0028663836419582367 \n",
            "Epoch: 191 | Loss: 0.0028252373449504375 \n",
            "Epoch: 192 | Loss: 0.0027846000157296658 \n",
            "Epoch: 193 | Loss: 0.00274460157379508 \n",
            "Epoch: 194 | Loss: 0.002705137012526393 \n",
            "Epoch: 195 | Loss: 0.0026662498712539673 \n",
            "Epoch: 196 | Loss: 0.0026279513258486986 \n",
            "Epoch: 197 | Loss: 0.002590199001133442 \n",
            "Epoch: 198 | Loss: 0.002552969381213188 \n",
            "Epoch: 199 | Loss: 0.002516257343813777 \n",
            "Epoch: 200 | Loss: 0.0024800975807011127 \n",
            "Epoch: 201 | Loss: 0.0024444586597383022 \n",
            "Epoch: 202 | Loss: 0.0024093342944979668 \n",
            "Epoch: 203 | Loss: 0.0023747188970446587 \n",
            "Epoch: 204 | Loss: 0.002340582199394703 \n",
            "Epoch: 205 | Loss: 0.00230694399215281 \n",
            "Epoch: 206 | Loss: 0.0022738014813512564 \n",
            "Epoch: 207 | Loss: 0.0022410994861274958 \n",
            "Epoch: 208 | Loss: 0.0022089052945375443 \n",
            "Epoch: 209 | Loss: 0.0021771444007754326 \n",
            "Epoch: 210 | Loss: 0.0021458561532199383 \n",
            "Epoch: 211 | Loss: 0.0021150268148630857 \n",
            "Epoch: 212 | Loss: 0.0020846277475357056 \n",
            "Epoch: 213 | Loss: 0.0020546570885926485 \n",
            "Epoch: 214 | Loss: 0.0020251465030014515 \n",
            "Epoch: 215 | Loss: 0.001996018225327134 \n",
            "Epoch: 216 | Loss: 0.0019673537462949753 \n",
            "Epoch: 217 | Loss: 0.001939068315550685 \n",
            "Epoch: 218 | Loss: 0.0019112087320536375 \n",
            "Epoch: 219 | Loss: 0.0018837274983525276 \n",
            "Epoch: 220 | Loss: 0.0018566906219348311 \n",
            "Epoch: 221 | Loss: 0.0018299785442650318 \n",
            "Epoch: 222 | Loss: 0.0018036772962659597 \n",
            "Epoch: 223 | Loss: 0.0017777577741071582 \n",
            "Epoch: 224 | Loss: 0.0017522011185064912 \n",
            "Epoch: 225 | Loss: 0.0017270297976210713 \n",
            "Epoch: 226 | Loss: 0.0017022015526890755 \n",
            "Epoch: 227 | Loss: 0.001677762484177947 \n",
            "Epoch: 228 | Loss: 0.0016536302864551544 \n",
            "Epoch: 229 | Loss: 0.0016298678237944841 \n",
            "Epoch: 230 | Loss: 0.00160644028801471 \n",
            "Epoch: 231 | Loss: 0.0015833629295229912 \n",
            "Epoch: 232 | Loss: 0.0015606086235493422 \n",
            "Epoch: 233 | Loss: 0.0015381717821583152 \n",
            "Epoch: 234 | Loss: 0.0015160785987973213 \n",
            "Epoch: 235 | Loss: 0.0014943003188818693 \n",
            "Epoch: 236 | Loss: 0.0014728134265169501 \n",
            "Epoch: 237 | Loss: 0.0014516415540128946 \n",
            "Epoch: 238 | Loss: 0.001430777250789106 \n",
            "Epoch: 239 | Loss: 0.0014102252898737788 \n",
            "Epoch: 240 | Loss: 0.0013899693731218576 \n",
            "Epoch: 241 | Loss: 0.0013699892442673445 \n",
            "Epoch: 242 | Loss: 0.0013502754736691713 \n",
            "Epoch: 243 | Loss: 0.0013308869674801826 \n",
            "Epoch: 244 | Loss: 0.0013117610942572355 \n",
            "Epoch: 245 | Loss: 0.0012928945943713188 \n",
            "Epoch: 246 | Loss: 0.0012743226252496243 \n",
            "Epoch: 247 | Loss: 0.0012560085160657763 \n",
            "Epoch: 248 | Loss: 0.0012379498220980167 \n",
            "Epoch: 249 | Loss: 0.0012201620265841484 \n",
            "Epoch: 250 | Loss: 0.0012026340700685978 \n",
            "Epoch: 251 | Loss: 0.0011853529140353203 \n",
            "Epoch: 252 | Loss: 0.0011683182092383504 \n",
            "Epoch: 253 | Loss: 0.001151536824181676 \n",
            "Epoch: 254 | Loss: 0.0011349651031196117 \n",
            "Epoch: 255 | Loss: 0.001118674874305725 \n",
            "Epoch: 256 | Loss: 0.0011025855783373117 \n",
            "Epoch: 257 | Loss: 0.001086744829080999 \n",
            "Epoch: 258 | Loss: 0.0010711272479966283 \n",
            "Epoch: 259 | Loss: 0.0010557286441326141 \n",
            "Epoch: 260 | Loss: 0.0010405650828033686 \n",
            "Epoch: 261 | Loss: 0.001025610021315515 \n",
            "Epoch: 262 | Loss: 0.0010108575224876404 \n",
            "Epoch: 263 | Loss: 0.0009963220218196511 \n",
            "Epoch: 264 | Loss: 0.0009820119012147188 \n",
            "Epoch: 265 | Loss: 0.0009679013164713979 \n",
            "Epoch: 266 | Loss: 0.000953989801928401 \n",
            "Epoch: 267 | Loss: 0.0009402681025676429 \n",
            "Epoch: 268 | Loss: 0.0009267802815884352 \n",
            "Epoch: 269 | Loss: 0.0009134444408118725 \n",
            "Epoch: 270 | Loss: 0.0009003186132758856 \n",
            "Epoch: 271 | Loss: 0.0008873813785612583 \n",
            "Epoch: 272 | Loss: 0.0008746253442950547 \n",
            "Epoch: 273 | Loss: 0.0008620641892775893 \n",
            "Epoch: 274 | Loss: 0.000849671836476773 \n",
            "Epoch: 275 | Loss: 0.0008374465978704393 \n",
            "Epoch: 276 | Loss: 0.0008254285785369575 \n",
            "Epoch: 277 | Loss: 0.0008135653915815055 \n",
            "Epoch: 278 | Loss: 0.0008018720545805991 \n",
            "Epoch: 279 | Loss: 0.0007903368677943945 \n",
            "Epoch: 280 | Loss: 0.0007789891678839922 \n",
            "Epoch: 281 | Loss: 0.0007677895482629538 \n",
            "Epoch: 282 | Loss: 0.0007567525608465075 \n",
            "Epoch: 283 | Loss: 0.0007458797190338373 \n",
            "Epoch: 284 | Loss: 0.000735154957510531 \n",
            "Epoch: 285 | Loss: 0.000724590674508363 \n",
            "Epoch: 286 | Loss: 0.000714174413587898 \n",
            "Epoch: 287 | Loss: 0.0007039199117571115 \n",
            "Epoch: 288 | Loss: 0.0006938023725524545 \n",
            "Epoch: 289 | Loss: 0.0006838250556029379 \n",
            "Epoch: 290 | Loss: 0.000673995353281498 \n",
            "Epoch: 291 | Loss: 0.0006643073284067214 \n",
            "Epoch: 292 | Loss: 0.0006547681987285614 \n",
            "Epoch: 293 | Loss: 0.0006453631212934852 \n",
            "Epoch: 294 | Loss: 0.000636081793345511 \n",
            "Epoch: 295 | Loss: 0.0006269419682212174 \n",
            "Epoch: 296 | Loss: 0.0006179246120154858 \n",
            "Epoch: 297 | Loss: 0.0006090457900427282 \n",
            "Epoch: 298 | Loss: 0.0006002924637869 \n",
            "Epoch: 299 | Loss: 0.0005916689406149089 \n",
            "Epoch: 300 | Loss: 0.0005831611924804747 \n",
            "Epoch: 301 | Loss: 0.0005747875547967851 \n",
            "Epoch: 302 | Loss: 0.0005665159551426768 \n",
            "Epoch: 303 | Loss: 0.0005583826568908989 \n",
            "Epoch: 304 | Loss: 0.0005503640277311206 \n",
            "Epoch: 305 | Loss: 0.0005424522096291184 \n",
            "Epoch: 306 | Loss: 0.0005346594844013453 \n",
            "Epoch: 307 | Loss: 0.0005269712419249117 \n",
            "Epoch: 308 | Loss: 0.0005193969700485468 \n",
            "Epoch: 309 | Loss: 0.0005119288107380271 \n",
            "Epoch: 310 | Loss: 0.000504581315908581 \n",
            "Epoch: 311 | Loss: 0.0004973271279595792 \n",
            "Epoch: 312 | Loss: 0.000490182195790112 \n",
            "Epoch: 313 | Loss: 0.0004831348778679967 \n",
            "Epoch: 314 | Loss: 0.0004761906457133591 \n",
            "Epoch: 315 | Loss: 0.0004693394876085222 \n",
            "Epoch: 316 | Loss: 0.0004625967994797975 \n",
            "Epoch: 317 | Loss: 0.0004559500084724277 \n",
            "Epoch: 318 | Loss: 0.00044940560474060476 \n",
            "Epoch: 319 | Loss: 0.0004429439431987703 \n",
            "Epoch: 320 | Loss: 0.000436575326602906 \n",
            "Epoch: 321 | Loss: 0.00043030339293181896 \n",
            "Epoch: 322 | Loss: 0.00042411760659888387 \n",
            "Epoch: 323 | Loss: 0.00041801820043474436 \n",
            "Epoch: 324 | Loss: 0.0004120195808354765 \n",
            "Epoch: 325 | Loss: 0.00040609482675790787 \n",
            "Epoch: 326 | Loss: 0.0004002571804448962 \n",
            "Epoch: 327 | Loss: 0.0003945068165194243 \n",
            "Epoch: 328 | Loss: 0.00038883130764588714 \n",
            "Epoch: 329 | Loss: 0.0003832333895843476 \n",
            "Epoch: 330 | Loss: 0.00037773928488604724 \n",
            "Epoch: 331 | Loss: 0.00037230394082143903 \n",
            "Epoch: 332 | Loss: 0.0003669592260848731 \n",
            "Epoch: 333 | Loss: 0.00036167638609185815 \n",
            "Epoch: 334 | Loss: 0.00035648891935124993 \n",
            "Epoch: 335 | Loss: 0.00035135308280587196 \n",
            "Epoch: 336 | Loss: 0.00034630540176294744 \n",
            "Epoch: 337 | Loss: 0.0003413360973354429 \n",
            "Epoch: 338 | Loss: 0.0003364229924045503 \n",
            "Epoch: 339 | Loss: 0.00033159737358801067 \n",
            "Epoch: 340 | Loss: 0.00032682661549188197 \n",
            "Epoch: 341 | Loss: 0.0003221300430595875 \n",
            "Epoch: 342 | Loss: 0.0003174933954142034 \n",
            "Epoch: 343 | Loss: 0.00031293253414332867 \n",
            "Epoch: 344 | Loss: 0.0003084475174546242 \n",
            "Epoch: 345 | Loss: 0.00030401413096114993 \n",
            "Epoch: 346 | Loss: 0.00029964203713461757 \n",
            "Epoch: 347 | Loss: 0.000295332632958889 \n",
            "Epoch: 348 | Loss: 0.00029108315357007086 \n",
            "Epoch: 349 | Loss: 0.0002869029121939093 \n",
            "Epoch: 350 | Loss: 0.0002827822172548622 \n",
            "Epoch: 351 | Loss: 0.00027871853671967983 \n",
            "Epoch: 352 | Loss: 0.0002747102698776871 \n",
            "Epoch: 353 | Loss: 0.00027076833066530526 \n",
            "Epoch: 354 | Loss: 0.00026687292847782373 \n",
            "Epoch: 355 | Loss: 0.0002630406816024333 \n",
            "Epoch: 356 | Loss: 0.0002592548553366214 \n",
            "Epoch: 357 | Loss: 0.0002555317769292742 \n",
            "Epoch: 358 | Loss: 0.00025185776758007705 \n",
            "Epoch: 359 | Loss: 0.00024824426509439945 \n",
            "Epoch: 360 | Loss: 0.00024466871400363743 \n",
            "Epoch: 361 | Loss: 0.00024116240092553198 \n",
            "Epoch: 362 | Loss: 0.00023769309336785227 \n",
            "Epoch: 363 | Loss: 0.00023427559062838554 \n",
            "Epoch: 364 | Loss: 0.0002309075789526105 \n",
            "Epoch: 365 | Loss: 0.00022758764680474997 \n",
            "Epoch: 366 | Loss: 0.0002243152557639405 \n",
            "Epoch: 367 | Loss: 0.00022109768178779632 \n",
            "Epoch: 368 | Loss: 0.0002179179573431611 \n",
            "Epoch: 369 | Loss: 0.00021478597773239017 \n",
            "Epoch: 370 | Loss: 0.00021170203399378806 \n",
            "Epoch: 371 | Loss: 0.00020866225531790406 \n",
            "Epoch: 372 | Loss: 0.0002056519442703575 \n",
            "Epoch: 373 | Loss: 0.0002027048758463934 \n",
            "Epoch: 374 | Loss: 0.0001997898070840165 \n",
            "Epoch: 375 | Loss: 0.00019691212219186127 \n",
            "Epoch: 376 | Loss: 0.00019408692605793476 \n",
            "Epoch: 377 | Loss: 0.00019129982683807611 \n",
            "Epoch: 378 | Loss: 0.0001885456731542945 \n",
            "Epoch: 379 | Loss: 0.0001858416071627289 \n",
            "Epoch: 380 | Loss: 0.00018316729983780533 \n",
            "Epoch: 381 | Loss: 0.00018053737585432827 \n",
            "Epoch: 382 | Loss: 0.0001779357553459704 \n",
            "Epoch: 383 | Loss: 0.0001753830729285255 \n",
            "Epoch: 384 | Loss: 0.0001728679344523698 \n",
            "Epoch: 385 | Loss: 0.00017038083751685917 \n",
            "Epoch: 386 | Loss: 0.00016792683163657784 \n",
            "Epoch: 387 | Loss: 0.00016551978478673846 \n",
            "Epoch: 388 | Loss: 0.00016313680680468678 \n",
            "Epoch: 389 | Loss: 0.0001607969170436263 \n",
            "Epoch: 390 | Loss: 0.00015848051407374442 \n",
            "Epoch: 391 | Loss: 0.0001561983663123101 \n",
            "Epoch: 392 | Loss: 0.00015395728405565023 \n",
            "Epoch: 393 | Loss: 0.00015174885629676282 \n",
            "Epoch: 394 | Loss: 0.00014956780069041997 \n",
            "Epoch: 395 | Loss: 0.00014741451013833284 \n",
            "Epoch: 396 | Loss: 0.00014529575128108263 \n",
            "Epoch: 397 | Loss: 0.0001432069402653724 \n",
            "Epoch: 398 | Loss: 0.00014115055091679096 \n",
            "Epoch: 399 | Loss: 0.00013912408030591905 \n",
            "Epoch: 400 | Loss: 0.00013712182408198714 \n",
            "Epoch: 401 | Loss: 0.00013515570026356727 \n",
            "Epoch: 402 | Loss: 0.00013321316509973258 \n",
            "Epoch: 403 | Loss: 0.00013129404396750033 \n",
            "Epoch: 404 | Loss: 0.00012940866872668266 \n",
            "Epoch: 405 | Loss: 0.00012755136413034052 \n",
            "Epoch: 406 | Loss: 0.0001257133699255064 \n",
            "Epoch: 407 | Loss: 0.00012390749179758132 \n",
            "Epoch: 408 | Loss: 0.00012212942237965763 \n",
            "Epoch: 409 | Loss: 0.00012037315173074603 \n",
            "Epoch: 410 | Loss: 0.0001186448207590729 \n",
            "Epoch: 411 | Loss: 0.00011694279965013266 \n",
            "Epoch: 412 | Loss: 0.0001152568293036893 \n",
            "Epoch: 413 | Loss: 0.00011360230564605445 \n",
            "Epoch: 414 | Loss: 0.00011196959530934691 \n",
            "Epoch: 415 | Loss: 0.00011036030628019944 \n",
            "Epoch: 416 | Loss: 0.00010877297609113157 \n",
            "Epoch: 417 | Loss: 0.0001072109880624339 \n",
            "Epoch: 418 | Loss: 0.00010567284334683791 \n",
            "Epoch: 419 | Loss: 0.00010415116412332281 \n",
            "Epoch: 420 | Loss: 0.00010265877062920481 \n",
            "Epoch: 421 | Loss: 0.00010117599595105276 \n",
            "Epoch: 422 | Loss: 9.972547559300438e-05 \n",
            "Epoch: 423 | Loss: 9.829407645156607e-05 \n",
            "Epoch: 424 | Loss: 9.687703277450055e-05 \n",
            "Epoch: 425 | Loss: 9.548960224492475e-05 \n",
            "Epoch: 426 | Loss: 9.411329665454105e-05 \n",
            "Epoch: 427 | Loss: 9.276658238377422e-05 \n",
            "Epoch: 428 | Loss: 9.143180068349466e-05 \n",
            "Epoch: 429 | Loss: 9.011167276185006e-05 \n",
            "Epoch: 430 | Loss: 8.882029942469671e-05 \n",
            "Epoch: 431 | Loss: 8.754315786063671e-05 \n",
            "Epoch: 432 | Loss: 8.628767682239413e-05 \n",
            "Epoch: 433 | Loss: 8.504449215251952e-05 \n",
            "Epoch: 434 | Loss: 8.382788655580953e-05 \n",
            "Epoch: 435 | Loss: 8.26232208055444e-05 \n",
            "Epoch: 436 | Loss: 8.143356535583735e-05 \n",
            "Epoch: 437 | Loss: 8.026557043194771e-05 \n",
            "Epoch: 438 | Loss: 7.910652493592352e-05 \n",
            "Epoch: 439 | Loss: 7.797028229106218e-05 \n",
            "Epoch: 440 | Loss: 7.684990123379976e-05 \n",
            "Epoch: 441 | Loss: 7.574573101010174e-05 \n",
            "Epoch: 442 | Loss: 7.4656585638877e-05 \n",
            "Epoch: 443 | Loss: 7.358728908002377e-05 \n",
            "Epoch: 444 | Loss: 7.2528186137788e-05 \n",
            "Epoch: 445 | Loss: 7.148561417125165e-05 \n",
            "Epoch: 446 | Loss: 7.045985694276169e-05 \n",
            "Epoch: 447 | Loss: 6.944248889340088e-05 \n",
            "Epoch: 448 | Loss: 6.8446941440925e-05 \n",
            "Epoch: 449 | Loss: 6.746574945282191e-05 \n",
            "Epoch: 450 | Loss: 6.649448914686218e-05 \n",
            "Epoch: 451 | Loss: 6.553874118253589e-05 \n",
            "Epoch: 452 | Loss: 6.459878932218999e-05 \n",
            "Epoch: 453 | Loss: 6.366654997691512e-05 \n",
            "Epoch: 454 | Loss: 6.275123450905085e-05 \n",
            "Epoch: 455 | Loss: 6.185258826008067e-05 \n",
            "Epoch: 456 | Loss: 6.0963611758779734e-05 \n",
            "Epoch: 457 | Loss: 6.008422133163549e-05 \n",
            "Epoch: 458 | Loss: 5.922598938923329e-05 \n",
            "Epoch: 459 | Loss: 5.837214848725125e-05 \n",
            "Epoch: 460 | Loss: 5.753201185143553e-05 \n",
            "Epoch: 461 | Loss: 5.6704531743889675e-05 \n",
            "Epoch: 462 | Loss: 5.589390639215708e-05 \n",
            "Epoch: 463 | Loss: 5.5084376072045416e-05 \n",
            "Epoch: 464 | Loss: 5.429660086520016e-05 \n",
            "Epoch: 465 | Loss: 5.351535219233483e-05 \n",
            "Epoch: 466 | Loss: 5.2749470341950655e-05 \n",
            "Epoch: 467 | Loss: 5.1985345635330305e-05 \n",
            "Epoch: 468 | Loss: 5.124467861605808e-05 \n",
            "Epoch: 469 | Loss: 5.050479376222938e-05 \n",
            "Epoch: 470 | Loss: 4.9778078391682357e-05 \n",
            "Epoch: 471 | Loss: 4.906314279651269e-05 \n",
            "Epoch: 472 | Loss: 4.836025618715212e-05 \n",
            "Epoch: 473 | Loss: 4.766485290019773e-05 \n",
            "Epoch: 474 | Loss: 4.697647091234103e-05 \n",
            "Epoch: 475 | Loss: 4.630298644769937e-05 \n",
            "Epoch: 476 | Loss: 4.564104892779142e-05 \n",
            "Epoch: 477 | Loss: 4.498270573094487e-05 \n",
            "Epoch: 478 | Loss: 4.433456342667341e-05 \n",
            "Epoch: 479 | Loss: 4.37011185567826e-05 \n",
            "Epoch: 480 | Loss: 4.30707041232381e-05 \n",
            "Epoch: 481 | Loss: 4.24524478148669e-05 \n",
            "Epoch: 482 | Loss: 4.1845054511213675e-05 \n",
            "Epoch: 483 | Loss: 4.1240920836571604e-05 \n",
            "Epoch: 484 | Loss: 4.0648592403158545e-05 \n",
            "Epoch: 485 | Loss: 4.006312519777566e-05 \n",
            "Epoch: 486 | Loss: 3.948811354348436e-05 \n",
            "Epoch: 487 | Loss: 3.8921611121622846e-05 \n",
            "Epoch: 488 | Loss: 3.835992538370192e-05 \n",
            "Epoch: 489 | Loss: 3.7808036722708493e-05 \n",
            "Epoch: 490 | Loss: 3.726405338966288e-05 \n",
            "Epoch: 491 | Loss: 3.673106402857229e-05 \n",
            "Epoch: 492 | Loss: 3.620365896495059e-05 \n",
            "Epoch: 493 | Loss: 3.5682154702953994e-05 \n",
            "Epoch: 494 | Loss: 3.517064033076167e-05 \n",
            "Epoch: 495 | Loss: 3.4664182749111205e-05 \n",
            "Epoch: 496 | Loss: 3.416480467421934e-05 \n",
            "Epoch: 497 | Loss: 3.367680619703606e-05 \n",
            "Epoch: 498 | Loss: 3.3191983675351366e-05 \n",
            "Epoch: 499 | Loss: 3.271565947216004e-05 \n",
            "Prediction (after training) 4 7.993424892425537\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # 0 gradients perform a backward pass + update the weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3659dbed",
      "metadata": {
        "id": "3659dbed"
      },
      "source": [
        "## 6 logistic_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5a216eb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a216eb4",
        "outputId": "229211fd-d07e-45a5-c398-73757ab008c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 | Loss: 1.8471\n",
            "Epoch 2/1000 | Loss: 1.8204\n",
            "Epoch 3/1000 | Loss: 1.7938\n",
            "Epoch 4/1000 | Loss: 1.7674\n",
            "Epoch 5/1000 | Loss: 1.7413\n",
            "Epoch 6/1000 | Loss: 1.7154\n",
            "Epoch 7/1000 | Loss: 1.6897\n",
            "Epoch 8/1000 | Loss: 1.6642\n",
            "Epoch 9/1000 | Loss: 1.6390\n",
            "Epoch 10/1000 | Loss: 1.6140\n",
            "Epoch 11/1000 | Loss: 1.5893\n",
            "Epoch 12/1000 | Loss: 1.5648\n",
            "Epoch 13/1000 | Loss: 1.5406\n",
            "Epoch 14/1000 | Loss: 1.5167\n",
            "Epoch 15/1000 | Loss: 1.4931\n",
            "Epoch 16/1000 | Loss: 1.4698\n",
            "Epoch 17/1000 | Loss: 1.4467\n",
            "Epoch 18/1000 | Loss: 1.4240\n",
            "Epoch 19/1000 | Loss: 1.4016\n",
            "Epoch 20/1000 | Loss: 1.3795\n",
            "Epoch 21/1000 | Loss: 1.3577\n",
            "Epoch 22/1000 | Loss: 1.3362\n",
            "Epoch 23/1000 | Loss: 1.3151\n",
            "Epoch 24/1000 | Loss: 1.2944\n",
            "Epoch 25/1000 | Loss: 1.2740\n",
            "Epoch 26/1000 | Loss: 1.2539\n",
            "Epoch 27/1000 | Loss: 1.2342\n",
            "Epoch 28/1000 | Loss: 1.2149\n",
            "Epoch 29/1000 | Loss: 1.1960\n",
            "Epoch 30/1000 | Loss: 1.1774\n",
            "Epoch 31/1000 | Loss: 1.1592\n",
            "Epoch 32/1000 | Loss: 1.1414\n",
            "Epoch 33/1000 | Loss: 1.1239\n",
            "Epoch 34/1000 | Loss: 1.1069\n",
            "Epoch 35/1000 | Loss: 1.0903\n",
            "Epoch 36/1000 | Loss: 1.0740\n",
            "Epoch 37/1000 | Loss: 1.0581\n",
            "Epoch 38/1000 | Loss: 1.0427\n",
            "Epoch 39/1000 | Loss: 1.0276\n",
            "Epoch 40/1000 | Loss: 1.0129\n",
            "Epoch 41/1000 | Loss: 0.9986\n",
            "Epoch 42/1000 | Loss: 0.9847\n",
            "Epoch 43/1000 | Loss: 0.9712\n",
            "Epoch 44/1000 | Loss: 0.9581\n",
            "Epoch 45/1000 | Loss: 0.9453\n",
            "Epoch 46/1000 | Loss: 0.9330\n",
            "Epoch 47/1000 | Loss: 0.9210\n",
            "Epoch 48/1000 | Loss: 0.9093\n",
            "Epoch 49/1000 | Loss: 0.8981\n",
            "Epoch 50/1000 | Loss: 0.8872\n",
            "Epoch 51/1000 | Loss: 0.8766\n",
            "Epoch 52/1000 | Loss: 0.8664\n",
            "Epoch 53/1000 | Loss: 0.8565\n",
            "Epoch 54/1000 | Loss: 0.8470\n",
            "Epoch 55/1000 | Loss: 0.8378\n",
            "Epoch 56/1000 | Loss: 0.8289\n",
            "Epoch 57/1000 | Loss: 0.8203\n",
            "Epoch 58/1000 | Loss: 0.8120\n",
            "Epoch 59/1000 | Loss: 0.8040\n",
            "Epoch 60/1000 | Loss: 0.7963\n",
            "Epoch 61/1000 | Loss: 0.7889\n",
            "Epoch 62/1000 | Loss: 0.7817\n",
            "Epoch 63/1000 | Loss: 0.7748\n",
            "Epoch 64/1000 | Loss: 0.7682\n",
            "Epoch 65/1000 | Loss: 0.7618\n",
            "Epoch 66/1000 | Loss: 0.7557\n",
            "Epoch 67/1000 | Loss: 0.7497\n",
            "Epoch 68/1000 | Loss: 0.7440\n",
            "Epoch 69/1000 | Loss: 0.7386\n",
            "Epoch 70/1000 | Loss: 0.7333\n",
            "Epoch 71/1000 | Loss: 0.7282\n",
            "Epoch 72/1000 | Loss: 0.7233\n",
            "Epoch 73/1000 | Loss: 0.7186\n",
            "Epoch 74/1000 | Loss: 0.7141\n",
            "Epoch 75/1000 | Loss: 0.7098\n",
            "Epoch 76/1000 | Loss: 0.7056\n",
            "Epoch 77/1000 | Loss: 0.7016\n",
            "Epoch 78/1000 | Loss: 0.6978\n",
            "Epoch 79/1000 | Loss: 0.6941\n",
            "Epoch 80/1000 | Loss: 0.6905\n",
            "Epoch 81/1000 | Loss: 0.6871\n",
            "Epoch 82/1000 | Loss: 0.6838\n",
            "Epoch 83/1000 | Loss: 0.6806\n",
            "Epoch 84/1000 | Loss: 0.6776\n",
            "Epoch 85/1000 | Loss: 0.6746\n",
            "Epoch 86/1000 | Loss: 0.6718\n",
            "Epoch 87/1000 | Loss: 0.6691\n",
            "Epoch 88/1000 | Loss: 0.6665\n",
            "Epoch 89/1000 | Loss: 0.6640\n",
            "Epoch 90/1000 | Loss: 0.6615\n",
            "Epoch 91/1000 | Loss: 0.6592\n",
            "Epoch 92/1000 | Loss: 0.6570\n",
            "Epoch 93/1000 | Loss: 0.6548\n",
            "Epoch 94/1000 | Loss: 0.6527\n",
            "Epoch 95/1000 | Loss: 0.6507\n",
            "Epoch 96/1000 | Loss: 0.6488\n",
            "Epoch 97/1000 | Loss: 0.6469\n",
            "Epoch 98/1000 | Loss: 0.6451\n",
            "Epoch 99/1000 | Loss: 0.6434\n",
            "Epoch 100/1000 | Loss: 0.6417\n",
            "Epoch 101/1000 | Loss: 0.6401\n",
            "Epoch 102/1000 | Loss: 0.6386\n",
            "Epoch 103/1000 | Loss: 0.6371\n",
            "Epoch 104/1000 | Loss: 0.6356\n",
            "Epoch 105/1000 | Loss: 0.6342\n",
            "Epoch 106/1000 | Loss: 0.6329\n",
            "Epoch 107/1000 | Loss: 0.6316\n",
            "Epoch 108/1000 | Loss: 0.6303\n",
            "Epoch 109/1000 | Loss: 0.6291\n",
            "Epoch 110/1000 | Loss: 0.6279\n",
            "Epoch 111/1000 | Loss: 0.6268\n",
            "Epoch 112/1000 | Loss: 0.6256\n",
            "Epoch 113/1000 | Loss: 0.6246\n",
            "Epoch 114/1000 | Loss: 0.6235\n",
            "Epoch 115/1000 | Loss: 0.6225\n",
            "Epoch 116/1000 | Loss: 0.6216\n",
            "Epoch 117/1000 | Loss: 0.6206\n",
            "Epoch 118/1000 | Loss: 0.6197\n",
            "Epoch 119/1000 | Loss: 0.6188\n",
            "Epoch 120/1000 | Loss: 0.6179\n",
            "Epoch 121/1000 | Loss: 0.6171\n",
            "Epoch 122/1000 | Loss: 0.6163\n",
            "Epoch 123/1000 | Loss: 0.6155\n",
            "Epoch 124/1000 | Loss: 0.6147\n",
            "Epoch 125/1000 | Loss: 0.6140\n",
            "Epoch 126/1000 | Loss: 0.6132\n",
            "Epoch 127/1000 | Loss: 0.6125\n",
            "Epoch 128/1000 | Loss: 0.6118\n",
            "Epoch 129/1000 | Loss: 0.6112\n",
            "Epoch 130/1000 | Loss: 0.6105\n",
            "Epoch 131/1000 | Loss: 0.6099\n",
            "Epoch 132/1000 | Loss: 0.6093\n",
            "Epoch 133/1000 | Loss: 0.6086\n",
            "Epoch 134/1000 | Loss: 0.6081\n",
            "Epoch 135/1000 | Loss: 0.6075\n",
            "Epoch 136/1000 | Loss: 0.6069\n",
            "Epoch 137/1000 | Loss: 0.6064\n",
            "Epoch 138/1000 | Loss: 0.6058\n",
            "Epoch 139/1000 | Loss: 0.6053\n",
            "Epoch 140/1000 | Loss: 0.6048\n",
            "Epoch 141/1000 | Loss: 0.6043\n",
            "Epoch 142/1000 | Loss: 0.6038\n",
            "Epoch 143/1000 | Loss: 0.6033\n",
            "Epoch 144/1000 | Loss: 0.6028\n",
            "Epoch 145/1000 | Loss: 0.6023\n",
            "Epoch 146/1000 | Loss: 0.6019\n",
            "Epoch 147/1000 | Loss: 0.6014\n",
            "Epoch 148/1000 | Loss: 0.6010\n",
            "Epoch 149/1000 | Loss: 0.6006\n",
            "Epoch 150/1000 | Loss: 0.6001\n",
            "Epoch 151/1000 | Loss: 0.5997\n",
            "Epoch 152/1000 | Loss: 0.5993\n",
            "Epoch 153/1000 | Loss: 0.5989\n",
            "Epoch 154/1000 | Loss: 0.5985\n",
            "Epoch 155/1000 | Loss: 0.5981\n",
            "Epoch 156/1000 | Loss: 0.5977\n",
            "Epoch 157/1000 | Loss: 0.5974\n",
            "Epoch 158/1000 | Loss: 0.5970\n",
            "Epoch 159/1000 | Loss: 0.5966\n",
            "Epoch 160/1000 | Loss: 0.5963\n",
            "Epoch 161/1000 | Loss: 0.5959\n",
            "Epoch 162/1000 | Loss: 0.5955\n",
            "Epoch 163/1000 | Loss: 0.5952\n",
            "Epoch 164/1000 | Loss: 0.5949\n",
            "Epoch 165/1000 | Loss: 0.5945\n",
            "Epoch 166/1000 | Loss: 0.5942\n",
            "Epoch 167/1000 | Loss: 0.5939\n",
            "Epoch 168/1000 | Loss: 0.5935\n",
            "Epoch 169/1000 | Loss: 0.5932\n",
            "Epoch 170/1000 | Loss: 0.5929\n",
            "Epoch 171/1000 | Loss: 0.5926\n",
            "Epoch 172/1000 | Loss: 0.5923\n",
            "Epoch 173/1000 | Loss: 0.5919\n",
            "Epoch 174/1000 | Loss: 0.5916\n",
            "Epoch 175/1000 | Loss: 0.5913\n",
            "Epoch 176/1000 | Loss: 0.5910\n",
            "Epoch 177/1000 | Loss: 0.5907\n",
            "Epoch 178/1000 | Loss: 0.5904\n",
            "Epoch 179/1000 | Loss: 0.5901\n",
            "Epoch 180/1000 | Loss: 0.5899\n",
            "Epoch 181/1000 | Loss: 0.5896\n",
            "Epoch 182/1000 | Loss: 0.5893\n",
            "Epoch 183/1000 | Loss: 0.5890\n",
            "Epoch 184/1000 | Loss: 0.5887\n",
            "Epoch 185/1000 | Loss: 0.5884\n",
            "Epoch 186/1000 | Loss: 0.5882\n",
            "Epoch 187/1000 | Loss: 0.5879\n",
            "Epoch 188/1000 | Loss: 0.5876\n",
            "Epoch 189/1000 | Loss: 0.5873\n",
            "Epoch 190/1000 | Loss: 0.5871\n",
            "Epoch 191/1000 | Loss: 0.5868\n",
            "Epoch 192/1000 | Loss: 0.5865\n",
            "Epoch 193/1000 | Loss: 0.5863\n",
            "Epoch 194/1000 | Loss: 0.5860\n",
            "Epoch 195/1000 | Loss: 0.5858\n",
            "Epoch 196/1000 | Loss: 0.5855\n",
            "Epoch 197/1000 | Loss: 0.5852\n",
            "Epoch 198/1000 | Loss: 0.5850\n",
            "Epoch 199/1000 | Loss: 0.5847\n",
            "Epoch 200/1000 | Loss: 0.5845\n",
            "Epoch 201/1000 | Loss: 0.5842\n",
            "Epoch 202/1000 | Loss: 0.5840\n",
            "Epoch 203/1000 | Loss: 0.5837\n",
            "Epoch 204/1000 | Loss: 0.5835\n",
            "Epoch 205/1000 | Loss: 0.5832\n",
            "Epoch 206/1000 | Loss: 0.5830\n",
            "Epoch 207/1000 | Loss: 0.5827\n",
            "Epoch 208/1000 | Loss: 0.5825\n",
            "Epoch 209/1000 | Loss: 0.5822\n",
            "Epoch 210/1000 | Loss: 0.5820\n",
            "Epoch 211/1000 | Loss: 0.5817\n",
            "Epoch 212/1000 | Loss: 0.5815\n",
            "Epoch 213/1000 | Loss: 0.5812\n",
            "Epoch 214/1000 | Loss: 0.5810\n",
            "Epoch 215/1000 | Loss: 0.5808\n",
            "Epoch 216/1000 | Loss: 0.5805\n",
            "Epoch 217/1000 | Loss: 0.5803\n",
            "Epoch 218/1000 | Loss: 0.5800\n",
            "Epoch 219/1000 | Loss: 0.5798\n",
            "Epoch 220/1000 | Loss: 0.5796\n",
            "Epoch 221/1000 | Loss: 0.5793\n",
            "Epoch 222/1000 | Loss: 0.5791\n",
            "Epoch 223/1000 | Loss: 0.5789\n",
            "Epoch 224/1000 | Loss: 0.5786\n",
            "Epoch 225/1000 | Loss: 0.5784\n",
            "Epoch 226/1000 | Loss: 0.5782\n",
            "Epoch 227/1000 | Loss: 0.5779\n",
            "Epoch 228/1000 | Loss: 0.5777\n",
            "Epoch 229/1000 | Loss: 0.5775\n",
            "Epoch 230/1000 | Loss: 0.5772\n",
            "Epoch 231/1000 | Loss: 0.5770\n",
            "Epoch 232/1000 | Loss: 0.5768\n",
            "Epoch 233/1000 | Loss: 0.5766\n",
            "Epoch 234/1000 | Loss: 0.5763\n",
            "Epoch 235/1000 | Loss: 0.5761\n",
            "Epoch 236/1000 | Loss: 0.5759\n",
            "Epoch 237/1000 | Loss: 0.5756\n",
            "Epoch 238/1000 | Loss: 0.5754\n",
            "Epoch 239/1000 | Loss: 0.5752\n",
            "Epoch 240/1000 | Loss: 0.5750\n",
            "Epoch 241/1000 | Loss: 0.5747\n",
            "Epoch 242/1000 | Loss: 0.5745\n",
            "Epoch 243/1000 | Loss: 0.5743\n",
            "Epoch 244/1000 | Loss: 0.5741\n",
            "Epoch 245/1000 | Loss: 0.5738\n",
            "Epoch 246/1000 | Loss: 0.5736\n",
            "Epoch 247/1000 | Loss: 0.5734\n",
            "Epoch 248/1000 | Loss: 0.5732\n",
            "Epoch 249/1000 | Loss: 0.5729\n",
            "Epoch 250/1000 | Loss: 0.5727\n",
            "Epoch 251/1000 | Loss: 0.5725\n",
            "Epoch 252/1000 | Loss: 0.5723\n",
            "Epoch 253/1000 | Loss: 0.5721\n",
            "Epoch 254/1000 | Loss: 0.5718\n",
            "Epoch 255/1000 | Loss: 0.5716\n",
            "Epoch 256/1000 | Loss: 0.5714\n",
            "Epoch 257/1000 | Loss: 0.5712\n",
            "Epoch 258/1000 | Loss: 0.5710\n",
            "Epoch 259/1000 | Loss: 0.5707\n",
            "Epoch 260/1000 | Loss: 0.5705\n",
            "Epoch 261/1000 | Loss: 0.5703\n",
            "Epoch 262/1000 | Loss: 0.5701\n",
            "Epoch 263/1000 | Loss: 0.5699\n",
            "Epoch 264/1000 | Loss: 0.5696\n",
            "Epoch 265/1000 | Loss: 0.5694\n",
            "Epoch 266/1000 | Loss: 0.5692\n",
            "Epoch 267/1000 | Loss: 0.5690\n",
            "Epoch 268/1000 | Loss: 0.5688\n",
            "Epoch 269/1000 | Loss: 0.5685\n",
            "Epoch 270/1000 | Loss: 0.5683\n",
            "Epoch 271/1000 | Loss: 0.5681\n",
            "Epoch 272/1000 | Loss: 0.5679\n",
            "Epoch 273/1000 | Loss: 0.5677\n",
            "Epoch 274/1000 | Loss: 0.5675\n",
            "Epoch 275/1000 | Loss: 0.5672\n",
            "Epoch 276/1000 | Loss: 0.5670\n",
            "Epoch 277/1000 | Loss: 0.5668\n",
            "Epoch 278/1000 | Loss: 0.5666\n",
            "Epoch 279/1000 | Loss: 0.5664\n",
            "Epoch 280/1000 | Loss: 0.5662\n",
            "Epoch 281/1000 | Loss: 0.5660\n",
            "Epoch 282/1000 | Loss: 0.5657\n",
            "Epoch 283/1000 | Loss: 0.5655\n",
            "Epoch 284/1000 | Loss: 0.5653\n",
            "Epoch 285/1000 | Loss: 0.5651\n",
            "Epoch 286/1000 | Loss: 0.5649\n",
            "Epoch 287/1000 | Loss: 0.5647\n",
            "Epoch 288/1000 | Loss: 0.5645\n",
            "Epoch 289/1000 | Loss: 0.5642\n",
            "Epoch 290/1000 | Loss: 0.5640\n",
            "Epoch 291/1000 | Loss: 0.5638\n",
            "Epoch 292/1000 | Loss: 0.5636\n",
            "Epoch 293/1000 | Loss: 0.5634\n",
            "Epoch 294/1000 | Loss: 0.5632\n",
            "Epoch 295/1000 | Loss: 0.5630\n",
            "Epoch 296/1000 | Loss: 0.5628\n",
            "Epoch 297/1000 | Loss: 0.5625\n",
            "Epoch 298/1000 | Loss: 0.5623\n",
            "Epoch 299/1000 | Loss: 0.5621\n",
            "Epoch 300/1000 | Loss: 0.5619\n",
            "Epoch 301/1000 | Loss: 0.5617\n",
            "Epoch 302/1000 | Loss: 0.5615\n",
            "Epoch 303/1000 | Loss: 0.5613\n",
            "Epoch 304/1000 | Loss: 0.5611\n",
            "Epoch 305/1000 | Loss: 0.5609\n",
            "Epoch 306/1000 | Loss: 0.5607\n",
            "Epoch 307/1000 | Loss: 0.5604\n",
            "Epoch 308/1000 | Loss: 0.5602\n",
            "Epoch 309/1000 | Loss: 0.5600\n",
            "Epoch 310/1000 | Loss: 0.5598\n",
            "Epoch 311/1000 | Loss: 0.5596\n",
            "Epoch 312/1000 | Loss: 0.5594\n",
            "Epoch 313/1000 | Loss: 0.5592\n",
            "Epoch 314/1000 | Loss: 0.5590\n",
            "Epoch 315/1000 | Loss: 0.5588\n",
            "Epoch 316/1000 | Loss: 0.5586\n",
            "Epoch 317/1000 | Loss: 0.5584\n",
            "Epoch 318/1000 | Loss: 0.5581\n",
            "Epoch 319/1000 | Loss: 0.5579\n",
            "Epoch 320/1000 | Loss: 0.5577\n",
            "Epoch 321/1000 | Loss: 0.5575\n",
            "Epoch 322/1000 | Loss: 0.5573\n",
            "Epoch 323/1000 | Loss: 0.5571\n",
            "Epoch 324/1000 | Loss: 0.5569\n",
            "Epoch 325/1000 | Loss: 0.5567\n",
            "Epoch 326/1000 | Loss: 0.5565\n",
            "Epoch 327/1000 | Loss: 0.5563\n",
            "Epoch 328/1000 | Loss: 0.5561\n",
            "Epoch 329/1000 | Loss: 0.5559\n",
            "Epoch 330/1000 | Loss: 0.5557\n",
            "Epoch 331/1000 | Loss: 0.5555\n",
            "Epoch 332/1000 | Loss: 0.5552\n",
            "Epoch 333/1000 | Loss: 0.5550\n",
            "Epoch 334/1000 | Loss: 0.5548\n",
            "Epoch 335/1000 | Loss: 0.5546\n",
            "Epoch 336/1000 | Loss: 0.5544\n",
            "Epoch 337/1000 | Loss: 0.5542\n",
            "Epoch 338/1000 | Loss: 0.5540\n",
            "Epoch 339/1000 | Loss: 0.5538\n",
            "Epoch 340/1000 | Loss: 0.5536\n",
            "Epoch 341/1000 | Loss: 0.5534\n",
            "Epoch 342/1000 | Loss: 0.5532\n",
            "Epoch 343/1000 | Loss: 0.5530\n",
            "Epoch 344/1000 | Loss: 0.5528\n",
            "Epoch 345/1000 | Loss: 0.5526\n",
            "Epoch 346/1000 | Loss: 0.5524\n",
            "Epoch 347/1000 | Loss: 0.5522\n",
            "Epoch 348/1000 | Loss: 0.5520\n",
            "Epoch 349/1000 | Loss: 0.5518\n",
            "Epoch 350/1000 | Loss: 0.5516\n",
            "Epoch 351/1000 | Loss: 0.5514\n",
            "Epoch 352/1000 | Loss: 0.5512\n",
            "Epoch 353/1000 | Loss: 0.5510\n",
            "Epoch 354/1000 | Loss: 0.5508\n",
            "Epoch 355/1000 | Loss: 0.5506\n",
            "Epoch 356/1000 | Loss: 0.5504\n",
            "Epoch 357/1000 | Loss: 0.5502\n",
            "Epoch 358/1000 | Loss: 0.5499\n",
            "Epoch 359/1000 | Loss: 0.5497\n",
            "Epoch 360/1000 | Loss: 0.5495\n",
            "Epoch 361/1000 | Loss: 0.5493\n",
            "Epoch 362/1000 | Loss: 0.5491\n",
            "Epoch 363/1000 | Loss: 0.5489\n",
            "Epoch 364/1000 | Loss: 0.5487\n",
            "Epoch 365/1000 | Loss: 0.5485\n",
            "Epoch 366/1000 | Loss: 0.5483\n",
            "Epoch 367/1000 | Loss: 0.5481\n",
            "Epoch 368/1000 | Loss: 0.5479\n",
            "Epoch 369/1000 | Loss: 0.5477\n",
            "Epoch 370/1000 | Loss: 0.5475\n",
            "Epoch 371/1000 | Loss: 0.5473\n",
            "Epoch 372/1000 | Loss: 0.5471\n",
            "Epoch 373/1000 | Loss: 0.5469\n",
            "Epoch 374/1000 | Loss: 0.5467\n",
            "Epoch 375/1000 | Loss: 0.5465\n",
            "Epoch 376/1000 | Loss: 0.5463\n",
            "Epoch 377/1000 | Loss: 0.5461\n",
            "Epoch 378/1000 | Loss: 0.5459\n",
            "Epoch 379/1000 | Loss: 0.5457\n",
            "Epoch 380/1000 | Loss: 0.5455\n",
            "Epoch 381/1000 | Loss: 0.5453\n",
            "Epoch 382/1000 | Loss: 0.5451\n",
            "Epoch 383/1000 | Loss: 0.5449\n",
            "Epoch 384/1000 | Loss: 0.5447\n",
            "Epoch 385/1000 | Loss: 0.5445\n",
            "Epoch 386/1000 | Loss: 0.5444\n",
            "Epoch 387/1000 | Loss: 0.5442\n",
            "Epoch 388/1000 | Loss: 0.5440\n",
            "Epoch 389/1000 | Loss: 0.5438\n",
            "Epoch 390/1000 | Loss: 0.5436\n",
            "Epoch 391/1000 | Loss: 0.5434\n",
            "Epoch 392/1000 | Loss: 0.5432\n",
            "Epoch 393/1000 | Loss: 0.5430\n",
            "Epoch 394/1000 | Loss: 0.5428\n",
            "Epoch 395/1000 | Loss: 0.5426\n",
            "Epoch 396/1000 | Loss: 0.5424\n",
            "Epoch 397/1000 | Loss: 0.5422\n",
            "Epoch 398/1000 | Loss: 0.5420\n",
            "Epoch 399/1000 | Loss: 0.5418\n",
            "Epoch 400/1000 | Loss: 0.5416\n",
            "Epoch 401/1000 | Loss: 0.5414\n",
            "Epoch 402/1000 | Loss: 0.5412\n",
            "Epoch 403/1000 | Loss: 0.5410\n",
            "Epoch 404/1000 | Loss: 0.5408\n",
            "Epoch 405/1000 | Loss: 0.5406\n",
            "Epoch 406/1000 | Loss: 0.5404\n",
            "Epoch 407/1000 | Loss: 0.5402\n",
            "Epoch 408/1000 | Loss: 0.5400\n",
            "Epoch 409/1000 | Loss: 0.5398\n",
            "Epoch 410/1000 | Loss: 0.5396\n",
            "Epoch 411/1000 | Loss: 0.5394\n",
            "Epoch 412/1000 | Loss: 0.5392\n",
            "Epoch 413/1000 | Loss: 0.5391\n",
            "Epoch 414/1000 | Loss: 0.5389\n",
            "Epoch 415/1000 | Loss: 0.5387\n",
            "Epoch 416/1000 | Loss: 0.5385\n",
            "Epoch 417/1000 | Loss: 0.5383\n",
            "Epoch 418/1000 | Loss: 0.5381\n",
            "Epoch 419/1000 | Loss: 0.5379\n",
            "Epoch 420/1000 | Loss: 0.5377\n",
            "Epoch 421/1000 | Loss: 0.5375\n",
            "Epoch 422/1000 | Loss: 0.5373\n",
            "Epoch 423/1000 | Loss: 0.5371\n",
            "Epoch 424/1000 | Loss: 0.5369\n",
            "Epoch 425/1000 | Loss: 0.5367\n",
            "Epoch 426/1000 | Loss: 0.5365\n",
            "Epoch 427/1000 | Loss: 0.5363\n",
            "Epoch 428/1000 | Loss: 0.5362\n",
            "Epoch 429/1000 | Loss: 0.5360\n",
            "Epoch 430/1000 | Loss: 0.5358\n",
            "Epoch 431/1000 | Loss: 0.5356\n",
            "Epoch 432/1000 | Loss: 0.5354\n",
            "Epoch 433/1000 | Loss: 0.5352\n",
            "Epoch 434/1000 | Loss: 0.5350\n",
            "Epoch 435/1000 | Loss: 0.5348\n",
            "Epoch 436/1000 | Loss: 0.5346\n",
            "Epoch 437/1000 | Loss: 0.5344\n",
            "Epoch 438/1000 | Loss: 0.5342\n",
            "Epoch 439/1000 | Loss: 0.5340\n",
            "Epoch 440/1000 | Loss: 0.5339\n",
            "Epoch 441/1000 | Loss: 0.5337\n",
            "Epoch 442/1000 | Loss: 0.5335\n",
            "Epoch 443/1000 | Loss: 0.5333\n",
            "Epoch 444/1000 | Loss: 0.5331\n",
            "Epoch 445/1000 | Loss: 0.5329\n",
            "Epoch 446/1000 | Loss: 0.5327\n",
            "Epoch 447/1000 | Loss: 0.5325\n",
            "Epoch 448/1000 | Loss: 0.5323\n",
            "Epoch 449/1000 | Loss: 0.5321\n",
            "Epoch 450/1000 | Loss: 0.5320\n",
            "Epoch 451/1000 | Loss: 0.5318\n",
            "Epoch 452/1000 | Loss: 0.5316\n",
            "Epoch 453/1000 | Loss: 0.5314\n",
            "Epoch 454/1000 | Loss: 0.5312\n",
            "Epoch 455/1000 | Loss: 0.5310\n",
            "Epoch 456/1000 | Loss: 0.5308\n",
            "Epoch 457/1000 | Loss: 0.5306\n",
            "Epoch 458/1000 | Loss: 0.5304\n",
            "Epoch 459/1000 | Loss: 0.5303\n",
            "Epoch 460/1000 | Loss: 0.5301\n",
            "Epoch 461/1000 | Loss: 0.5299\n",
            "Epoch 462/1000 | Loss: 0.5297\n",
            "Epoch 463/1000 | Loss: 0.5295\n",
            "Epoch 464/1000 | Loss: 0.5293\n",
            "Epoch 465/1000 | Loss: 0.5291\n",
            "Epoch 466/1000 | Loss: 0.5289\n",
            "Epoch 467/1000 | Loss: 0.5288\n",
            "Epoch 468/1000 | Loss: 0.5286\n",
            "Epoch 469/1000 | Loss: 0.5284\n",
            "Epoch 470/1000 | Loss: 0.5282\n",
            "Epoch 471/1000 | Loss: 0.5280\n",
            "Epoch 472/1000 | Loss: 0.5278\n",
            "Epoch 473/1000 | Loss: 0.5276\n",
            "Epoch 474/1000 | Loss: 0.5274\n",
            "Epoch 475/1000 | Loss: 0.5273\n",
            "Epoch 476/1000 | Loss: 0.5271\n",
            "Epoch 477/1000 | Loss: 0.5269\n",
            "Epoch 478/1000 | Loss: 0.5267\n",
            "Epoch 479/1000 | Loss: 0.5265\n",
            "Epoch 480/1000 | Loss: 0.5263\n",
            "Epoch 481/1000 | Loss: 0.5261\n",
            "Epoch 482/1000 | Loss: 0.5260\n",
            "Epoch 483/1000 | Loss: 0.5258\n",
            "Epoch 484/1000 | Loss: 0.5256\n",
            "Epoch 485/1000 | Loss: 0.5254\n",
            "Epoch 486/1000 | Loss: 0.5252\n",
            "Epoch 487/1000 | Loss: 0.5250\n",
            "Epoch 488/1000 | Loss: 0.5249\n",
            "Epoch 489/1000 | Loss: 0.5247\n",
            "Epoch 490/1000 | Loss: 0.5245\n",
            "Epoch 491/1000 | Loss: 0.5243\n",
            "Epoch 492/1000 | Loss: 0.5241\n",
            "Epoch 493/1000 | Loss: 0.5239\n",
            "Epoch 494/1000 | Loss: 0.5237\n",
            "Epoch 495/1000 | Loss: 0.5236\n",
            "Epoch 496/1000 | Loss: 0.5234\n",
            "Epoch 497/1000 | Loss: 0.5232\n",
            "Epoch 498/1000 | Loss: 0.5230\n",
            "Epoch 499/1000 | Loss: 0.5228\n",
            "Epoch 500/1000 | Loss: 0.5226\n",
            "Epoch 501/1000 | Loss: 0.5225\n",
            "Epoch 502/1000 | Loss: 0.5223\n",
            "Epoch 503/1000 | Loss: 0.5221\n",
            "Epoch 504/1000 | Loss: 0.5219\n",
            "Epoch 505/1000 | Loss: 0.5217\n",
            "Epoch 506/1000 | Loss: 0.5216\n",
            "Epoch 507/1000 | Loss: 0.5214\n",
            "Epoch 508/1000 | Loss: 0.5212\n",
            "Epoch 509/1000 | Loss: 0.5210\n",
            "Epoch 510/1000 | Loss: 0.5208\n",
            "Epoch 511/1000 | Loss: 0.5206\n",
            "Epoch 512/1000 | Loss: 0.5205\n",
            "Epoch 513/1000 | Loss: 0.5203\n",
            "Epoch 514/1000 | Loss: 0.5201\n",
            "Epoch 515/1000 | Loss: 0.5199\n",
            "Epoch 516/1000 | Loss: 0.5197\n",
            "Epoch 517/1000 | Loss: 0.5196\n",
            "Epoch 518/1000 | Loss: 0.5194\n",
            "Epoch 519/1000 | Loss: 0.5192\n",
            "Epoch 520/1000 | Loss: 0.5190\n",
            "Epoch 521/1000 | Loss: 0.5188\n",
            "Epoch 522/1000 | Loss: 0.5187\n",
            "Epoch 523/1000 | Loss: 0.5185\n",
            "Epoch 524/1000 | Loss: 0.5183\n",
            "Epoch 525/1000 | Loss: 0.5181\n",
            "Epoch 526/1000 | Loss: 0.5179\n",
            "Epoch 527/1000 | Loss: 0.5178\n",
            "Epoch 528/1000 | Loss: 0.5176\n",
            "Epoch 529/1000 | Loss: 0.5174\n",
            "Epoch 530/1000 | Loss: 0.5172\n",
            "Epoch 531/1000 | Loss: 0.5170\n",
            "Epoch 532/1000 | Loss: 0.5169\n",
            "Epoch 533/1000 | Loss: 0.5167\n",
            "Epoch 534/1000 | Loss: 0.5165\n",
            "Epoch 535/1000 | Loss: 0.5163\n",
            "Epoch 536/1000 | Loss: 0.5161\n",
            "Epoch 537/1000 | Loss: 0.5160\n",
            "Epoch 538/1000 | Loss: 0.5158\n",
            "Epoch 539/1000 | Loss: 0.5156\n",
            "Epoch 540/1000 | Loss: 0.5154\n",
            "Epoch 541/1000 | Loss: 0.5153\n",
            "Epoch 542/1000 | Loss: 0.5151\n",
            "Epoch 543/1000 | Loss: 0.5149\n",
            "Epoch 544/1000 | Loss: 0.5147\n",
            "Epoch 545/1000 | Loss: 0.5145\n",
            "Epoch 546/1000 | Loss: 0.5144\n",
            "Epoch 547/1000 | Loss: 0.5142\n",
            "Epoch 548/1000 | Loss: 0.5140\n",
            "Epoch 549/1000 | Loss: 0.5138\n",
            "Epoch 550/1000 | Loss: 0.5137\n",
            "Epoch 551/1000 | Loss: 0.5135\n",
            "Epoch 552/1000 | Loss: 0.5133\n",
            "Epoch 553/1000 | Loss: 0.5131\n",
            "Epoch 554/1000 | Loss: 0.5129\n",
            "Epoch 555/1000 | Loss: 0.5128\n",
            "Epoch 556/1000 | Loss: 0.5126\n",
            "Epoch 557/1000 | Loss: 0.5124\n",
            "Epoch 558/1000 | Loss: 0.5122\n",
            "Epoch 559/1000 | Loss: 0.5121\n",
            "Epoch 560/1000 | Loss: 0.5119\n",
            "Epoch 561/1000 | Loss: 0.5117\n",
            "Epoch 562/1000 | Loss: 0.5115\n",
            "Epoch 563/1000 | Loss: 0.5114\n",
            "Epoch 564/1000 | Loss: 0.5112\n",
            "Epoch 565/1000 | Loss: 0.5110\n",
            "Epoch 566/1000 | Loss: 0.5108\n",
            "Epoch 567/1000 | Loss: 0.5107\n",
            "Epoch 568/1000 | Loss: 0.5105\n",
            "Epoch 569/1000 | Loss: 0.5103\n",
            "Epoch 570/1000 | Loss: 0.5101\n",
            "Epoch 571/1000 | Loss: 0.5100\n",
            "Epoch 572/1000 | Loss: 0.5098\n",
            "Epoch 573/1000 | Loss: 0.5096\n",
            "Epoch 574/1000 | Loss: 0.5094\n",
            "Epoch 575/1000 | Loss: 0.5093\n",
            "Epoch 576/1000 | Loss: 0.5091\n",
            "Epoch 577/1000 | Loss: 0.5089\n",
            "Epoch 578/1000 | Loss: 0.5088\n",
            "Epoch 579/1000 | Loss: 0.5086\n",
            "Epoch 580/1000 | Loss: 0.5084\n",
            "Epoch 581/1000 | Loss: 0.5082\n",
            "Epoch 582/1000 | Loss: 0.5081\n",
            "Epoch 583/1000 | Loss: 0.5079\n",
            "Epoch 584/1000 | Loss: 0.5077\n",
            "Epoch 585/1000 | Loss: 0.5075\n",
            "Epoch 586/1000 | Loss: 0.5074\n",
            "Epoch 587/1000 | Loss: 0.5072\n",
            "Epoch 588/1000 | Loss: 0.5070\n",
            "Epoch 589/1000 | Loss: 0.5068\n",
            "Epoch 590/1000 | Loss: 0.5067\n",
            "Epoch 591/1000 | Loss: 0.5065\n",
            "Epoch 592/1000 | Loss: 0.5063\n",
            "Epoch 593/1000 | Loss: 0.5062\n",
            "Epoch 594/1000 | Loss: 0.5060\n",
            "Epoch 595/1000 | Loss: 0.5058\n",
            "Epoch 596/1000 | Loss: 0.5056\n",
            "Epoch 597/1000 | Loss: 0.5055\n",
            "Epoch 598/1000 | Loss: 0.5053\n",
            "Epoch 599/1000 | Loss: 0.5051\n",
            "Epoch 600/1000 | Loss: 0.5050\n",
            "Epoch 601/1000 | Loss: 0.5048\n",
            "Epoch 602/1000 | Loss: 0.5046\n",
            "Epoch 603/1000 | Loss: 0.5045\n",
            "Epoch 604/1000 | Loss: 0.5043\n",
            "Epoch 605/1000 | Loss: 0.5041\n",
            "Epoch 606/1000 | Loss: 0.5039\n",
            "Epoch 607/1000 | Loss: 0.5038\n",
            "Epoch 608/1000 | Loss: 0.5036\n",
            "Epoch 609/1000 | Loss: 0.5034\n",
            "Epoch 610/1000 | Loss: 0.5033\n",
            "Epoch 611/1000 | Loss: 0.5031\n",
            "Epoch 612/1000 | Loss: 0.5029\n",
            "Epoch 613/1000 | Loss: 0.5028\n",
            "Epoch 614/1000 | Loss: 0.5026\n",
            "Epoch 615/1000 | Loss: 0.5024\n",
            "Epoch 616/1000 | Loss: 0.5022\n",
            "Epoch 617/1000 | Loss: 0.5021\n",
            "Epoch 618/1000 | Loss: 0.5019\n",
            "Epoch 619/1000 | Loss: 0.5017\n",
            "Epoch 620/1000 | Loss: 0.5016\n",
            "Epoch 621/1000 | Loss: 0.5014\n",
            "Epoch 622/1000 | Loss: 0.5012\n",
            "Epoch 623/1000 | Loss: 0.5011\n",
            "Epoch 624/1000 | Loss: 0.5009\n",
            "Epoch 625/1000 | Loss: 0.5007\n",
            "Epoch 626/1000 | Loss: 0.5006\n",
            "Epoch 627/1000 | Loss: 0.5004\n",
            "Epoch 628/1000 | Loss: 0.5002\n",
            "Epoch 629/1000 | Loss: 0.5001\n",
            "Epoch 630/1000 | Loss: 0.4999\n",
            "Epoch 631/1000 | Loss: 0.4997\n",
            "Epoch 632/1000 | Loss: 0.4996\n",
            "Epoch 633/1000 | Loss: 0.4994\n",
            "Epoch 634/1000 | Loss: 0.4992\n",
            "Epoch 635/1000 | Loss: 0.4991\n",
            "Epoch 636/1000 | Loss: 0.4989\n",
            "Epoch 637/1000 | Loss: 0.4987\n",
            "Epoch 638/1000 | Loss: 0.4986\n",
            "Epoch 639/1000 | Loss: 0.4984\n",
            "Epoch 640/1000 | Loss: 0.4982\n",
            "Epoch 641/1000 | Loss: 0.4981\n",
            "Epoch 642/1000 | Loss: 0.4979\n",
            "Epoch 643/1000 | Loss: 0.4977\n",
            "Epoch 644/1000 | Loss: 0.4976\n",
            "Epoch 645/1000 | Loss: 0.4974\n",
            "Epoch 646/1000 | Loss: 0.4972\n",
            "Epoch 647/1000 | Loss: 0.4971\n",
            "Epoch 648/1000 | Loss: 0.4969\n",
            "Epoch 649/1000 | Loss: 0.4967\n",
            "Epoch 650/1000 | Loss: 0.4966\n",
            "Epoch 651/1000 | Loss: 0.4964\n",
            "Epoch 652/1000 | Loss: 0.4962\n",
            "Epoch 653/1000 | Loss: 0.4961\n",
            "Epoch 654/1000 | Loss: 0.4959\n",
            "Epoch 655/1000 | Loss: 0.4957\n",
            "Epoch 656/1000 | Loss: 0.4956\n",
            "Epoch 657/1000 | Loss: 0.4954\n",
            "Epoch 658/1000 | Loss: 0.4952\n",
            "Epoch 659/1000 | Loss: 0.4951\n",
            "Epoch 660/1000 | Loss: 0.4949\n",
            "Epoch 661/1000 | Loss: 0.4947\n",
            "Epoch 662/1000 | Loss: 0.4946\n",
            "Epoch 663/1000 | Loss: 0.4944\n",
            "Epoch 664/1000 | Loss: 0.4943\n",
            "Epoch 665/1000 | Loss: 0.4941\n",
            "Epoch 666/1000 | Loss: 0.4939\n",
            "Epoch 667/1000 | Loss: 0.4938\n",
            "Epoch 668/1000 | Loss: 0.4936\n",
            "Epoch 669/1000 | Loss: 0.4934\n",
            "Epoch 670/1000 | Loss: 0.4933\n",
            "Epoch 671/1000 | Loss: 0.4931\n",
            "Epoch 672/1000 | Loss: 0.4930\n",
            "Epoch 673/1000 | Loss: 0.4928\n",
            "Epoch 674/1000 | Loss: 0.4926\n",
            "Epoch 675/1000 | Loss: 0.4925\n",
            "Epoch 676/1000 | Loss: 0.4923\n",
            "Epoch 677/1000 | Loss: 0.4921\n",
            "Epoch 678/1000 | Loss: 0.4920\n",
            "Epoch 679/1000 | Loss: 0.4918\n",
            "Epoch 680/1000 | Loss: 0.4917\n",
            "Epoch 681/1000 | Loss: 0.4915\n",
            "Epoch 682/1000 | Loss: 0.4913\n",
            "Epoch 683/1000 | Loss: 0.4912\n",
            "Epoch 684/1000 | Loss: 0.4910\n",
            "Epoch 685/1000 | Loss: 0.4908\n",
            "Epoch 686/1000 | Loss: 0.4907\n",
            "Epoch 687/1000 | Loss: 0.4905\n",
            "Epoch 688/1000 | Loss: 0.4904\n",
            "Epoch 689/1000 | Loss: 0.4902\n",
            "Epoch 690/1000 | Loss: 0.4900\n",
            "Epoch 691/1000 | Loss: 0.4899\n",
            "Epoch 692/1000 | Loss: 0.4897\n",
            "Epoch 693/1000 | Loss: 0.4896\n",
            "Epoch 694/1000 | Loss: 0.4894\n",
            "Epoch 695/1000 | Loss: 0.4892\n",
            "Epoch 696/1000 | Loss: 0.4891\n",
            "Epoch 697/1000 | Loss: 0.4889\n",
            "Epoch 698/1000 | Loss: 0.4888\n",
            "Epoch 699/1000 | Loss: 0.4886\n",
            "Epoch 700/1000 | Loss: 0.4884\n",
            "Epoch 701/1000 | Loss: 0.4883\n",
            "Epoch 702/1000 | Loss: 0.4881\n",
            "Epoch 703/1000 | Loss: 0.4880\n",
            "Epoch 704/1000 | Loss: 0.4878\n",
            "Epoch 705/1000 | Loss: 0.4876\n",
            "Epoch 706/1000 | Loss: 0.4875\n",
            "Epoch 707/1000 | Loss: 0.4873\n",
            "Epoch 708/1000 | Loss: 0.4872\n",
            "Epoch 709/1000 | Loss: 0.4870\n",
            "Epoch 710/1000 | Loss: 0.4868\n",
            "Epoch 711/1000 | Loss: 0.4867\n",
            "Epoch 712/1000 | Loss: 0.4865\n",
            "Epoch 713/1000 | Loss: 0.4864\n",
            "Epoch 714/1000 | Loss: 0.4862\n",
            "Epoch 715/1000 | Loss: 0.4861\n",
            "Epoch 716/1000 | Loss: 0.4859\n",
            "Epoch 717/1000 | Loss: 0.4857\n",
            "Epoch 718/1000 | Loss: 0.4856\n",
            "Epoch 719/1000 | Loss: 0.4854\n",
            "Epoch 720/1000 | Loss: 0.4853\n",
            "Epoch 721/1000 | Loss: 0.4851\n",
            "Epoch 722/1000 | Loss: 0.4849\n",
            "Epoch 723/1000 | Loss: 0.4848\n",
            "Epoch 724/1000 | Loss: 0.4846\n",
            "Epoch 725/1000 | Loss: 0.4845\n",
            "Epoch 726/1000 | Loss: 0.4843\n",
            "Epoch 727/1000 | Loss: 0.4842\n",
            "Epoch 728/1000 | Loss: 0.4840\n",
            "Epoch 729/1000 | Loss: 0.4838\n",
            "Epoch 730/1000 | Loss: 0.4837\n",
            "Epoch 731/1000 | Loss: 0.4835\n",
            "Epoch 732/1000 | Loss: 0.4834\n",
            "Epoch 733/1000 | Loss: 0.4832\n",
            "Epoch 734/1000 | Loss: 0.4831\n",
            "Epoch 735/1000 | Loss: 0.4829\n",
            "Epoch 736/1000 | Loss: 0.4828\n",
            "Epoch 737/1000 | Loss: 0.4826\n",
            "Epoch 738/1000 | Loss: 0.4824\n",
            "Epoch 739/1000 | Loss: 0.4823\n",
            "Epoch 740/1000 | Loss: 0.4821\n",
            "Epoch 741/1000 | Loss: 0.4820\n",
            "Epoch 742/1000 | Loss: 0.4818\n",
            "Epoch 743/1000 | Loss: 0.4817\n",
            "Epoch 744/1000 | Loss: 0.4815\n",
            "Epoch 745/1000 | Loss: 0.4814\n",
            "Epoch 746/1000 | Loss: 0.4812\n",
            "Epoch 747/1000 | Loss: 0.4810\n",
            "Epoch 748/1000 | Loss: 0.4809\n",
            "Epoch 749/1000 | Loss: 0.4807\n",
            "Epoch 750/1000 | Loss: 0.4806\n",
            "Epoch 751/1000 | Loss: 0.4804\n",
            "Epoch 752/1000 | Loss: 0.4803\n",
            "Epoch 753/1000 | Loss: 0.4801\n",
            "Epoch 754/1000 | Loss: 0.4800\n",
            "Epoch 755/1000 | Loss: 0.4798\n",
            "Epoch 756/1000 | Loss: 0.4797\n",
            "Epoch 757/1000 | Loss: 0.4795\n",
            "Epoch 758/1000 | Loss: 0.4793\n",
            "Epoch 759/1000 | Loss: 0.4792\n",
            "Epoch 760/1000 | Loss: 0.4790\n",
            "Epoch 761/1000 | Loss: 0.4789\n",
            "Epoch 762/1000 | Loss: 0.4787\n",
            "Epoch 763/1000 | Loss: 0.4786\n",
            "Epoch 764/1000 | Loss: 0.4784\n",
            "Epoch 765/1000 | Loss: 0.4783\n",
            "Epoch 766/1000 | Loss: 0.4781\n",
            "Epoch 767/1000 | Loss: 0.4780\n",
            "Epoch 768/1000 | Loss: 0.4778\n",
            "Epoch 769/1000 | Loss: 0.4777\n",
            "Epoch 770/1000 | Loss: 0.4775\n",
            "Epoch 771/1000 | Loss: 0.4774\n",
            "Epoch 772/1000 | Loss: 0.4772\n",
            "Epoch 773/1000 | Loss: 0.4771\n",
            "Epoch 774/1000 | Loss: 0.4769\n",
            "Epoch 775/1000 | Loss: 0.4767\n",
            "Epoch 776/1000 | Loss: 0.4766\n",
            "Epoch 777/1000 | Loss: 0.4764\n",
            "Epoch 778/1000 | Loss: 0.4763\n",
            "Epoch 779/1000 | Loss: 0.4761\n",
            "Epoch 780/1000 | Loss: 0.4760\n",
            "Epoch 781/1000 | Loss: 0.4758\n",
            "Epoch 782/1000 | Loss: 0.4757\n",
            "Epoch 783/1000 | Loss: 0.4755\n",
            "Epoch 784/1000 | Loss: 0.4754\n",
            "Epoch 785/1000 | Loss: 0.4752\n",
            "Epoch 786/1000 | Loss: 0.4751\n",
            "Epoch 787/1000 | Loss: 0.4749\n",
            "Epoch 788/1000 | Loss: 0.4748\n",
            "Epoch 789/1000 | Loss: 0.4746\n",
            "Epoch 790/1000 | Loss: 0.4745\n",
            "Epoch 791/1000 | Loss: 0.4743\n",
            "Epoch 792/1000 | Loss: 0.4742\n",
            "Epoch 793/1000 | Loss: 0.4740\n",
            "Epoch 794/1000 | Loss: 0.4739\n",
            "Epoch 795/1000 | Loss: 0.4737\n",
            "Epoch 796/1000 | Loss: 0.4736\n",
            "Epoch 797/1000 | Loss: 0.4734\n",
            "Epoch 798/1000 | Loss: 0.4733\n",
            "Epoch 799/1000 | Loss: 0.4731\n",
            "Epoch 800/1000 | Loss: 0.4730\n",
            "Epoch 801/1000 | Loss: 0.4728\n",
            "Epoch 802/1000 | Loss: 0.4727\n",
            "Epoch 803/1000 | Loss: 0.4725\n",
            "Epoch 804/1000 | Loss: 0.4724\n",
            "Epoch 805/1000 | Loss: 0.4722\n",
            "Epoch 806/1000 | Loss: 0.4721\n",
            "Epoch 807/1000 | Loss: 0.4719\n",
            "Epoch 808/1000 | Loss: 0.4718\n",
            "Epoch 809/1000 | Loss: 0.4716\n",
            "Epoch 810/1000 | Loss: 0.4715\n",
            "Epoch 811/1000 | Loss: 0.4713\n",
            "Epoch 812/1000 | Loss: 0.4712\n",
            "Epoch 813/1000 | Loss: 0.4710\n",
            "Epoch 814/1000 | Loss: 0.4709\n",
            "Epoch 815/1000 | Loss: 0.4707\n",
            "Epoch 816/1000 | Loss: 0.4706\n",
            "Epoch 817/1000 | Loss: 0.4705\n",
            "Epoch 818/1000 | Loss: 0.4703\n",
            "Epoch 819/1000 | Loss: 0.4702\n",
            "Epoch 820/1000 | Loss: 0.4700\n",
            "Epoch 821/1000 | Loss: 0.4699\n",
            "Epoch 822/1000 | Loss: 0.4697\n",
            "Epoch 823/1000 | Loss: 0.4696\n",
            "Epoch 824/1000 | Loss: 0.4694\n",
            "Epoch 825/1000 | Loss: 0.4693\n",
            "Epoch 826/1000 | Loss: 0.4691\n",
            "Epoch 827/1000 | Loss: 0.4690\n",
            "Epoch 828/1000 | Loss: 0.4688\n",
            "Epoch 829/1000 | Loss: 0.4687\n",
            "Epoch 830/1000 | Loss: 0.4685\n",
            "Epoch 831/1000 | Loss: 0.4684\n",
            "Epoch 832/1000 | Loss: 0.4682\n",
            "Epoch 833/1000 | Loss: 0.4681\n",
            "Epoch 834/1000 | Loss: 0.4680\n",
            "Epoch 835/1000 | Loss: 0.4678\n",
            "Epoch 836/1000 | Loss: 0.4677\n",
            "Epoch 837/1000 | Loss: 0.4675\n",
            "Epoch 838/1000 | Loss: 0.4674\n",
            "Epoch 839/1000 | Loss: 0.4672\n",
            "Epoch 840/1000 | Loss: 0.4671\n",
            "Epoch 841/1000 | Loss: 0.4669\n",
            "Epoch 842/1000 | Loss: 0.4668\n",
            "Epoch 843/1000 | Loss: 0.4666\n",
            "Epoch 844/1000 | Loss: 0.4665\n",
            "Epoch 845/1000 | Loss: 0.4664\n",
            "Epoch 846/1000 | Loss: 0.4662\n",
            "Epoch 847/1000 | Loss: 0.4661\n",
            "Epoch 848/1000 | Loss: 0.4659\n",
            "Epoch 849/1000 | Loss: 0.4658\n",
            "Epoch 850/1000 | Loss: 0.4656\n",
            "Epoch 851/1000 | Loss: 0.4655\n",
            "Epoch 852/1000 | Loss: 0.4653\n",
            "Epoch 853/1000 | Loss: 0.4652\n",
            "Epoch 854/1000 | Loss: 0.4650\n",
            "Epoch 855/1000 | Loss: 0.4649\n",
            "Epoch 856/1000 | Loss: 0.4648\n",
            "Epoch 857/1000 | Loss: 0.4646\n",
            "Epoch 858/1000 | Loss: 0.4645\n",
            "Epoch 859/1000 | Loss: 0.4643\n",
            "Epoch 860/1000 | Loss: 0.4642\n",
            "Epoch 861/1000 | Loss: 0.4640\n",
            "Epoch 862/1000 | Loss: 0.4639\n",
            "Epoch 863/1000 | Loss: 0.4638\n",
            "Epoch 864/1000 | Loss: 0.4636\n",
            "Epoch 865/1000 | Loss: 0.4635\n",
            "Epoch 866/1000 | Loss: 0.4633\n",
            "Epoch 867/1000 | Loss: 0.4632\n",
            "Epoch 868/1000 | Loss: 0.4630\n",
            "Epoch 869/1000 | Loss: 0.4629\n",
            "Epoch 870/1000 | Loss: 0.4628\n",
            "Epoch 871/1000 | Loss: 0.4626\n",
            "Epoch 872/1000 | Loss: 0.4625\n",
            "Epoch 873/1000 | Loss: 0.4623\n",
            "Epoch 874/1000 | Loss: 0.4622\n",
            "Epoch 875/1000 | Loss: 0.4620\n",
            "Epoch 876/1000 | Loss: 0.4619\n",
            "Epoch 877/1000 | Loss: 0.4618\n",
            "Epoch 878/1000 | Loss: 0.4616\n",
            "Epoch 879/1000 | Loss: 0.4615\n",
            "Epoch 880/1000 | Loss: 0.4613\n",
            "Epoch 881/1000 | Loss: 0.4612\n",
            "Epoch 882/1000 | Loss: 0.4610\n",
            "Epoch 883/1000 | Loss: 0.4609\n",
            "Epoch 884/1000 | Loss: 0.4608\n",
            "Epoch 885/1000 | Loss: 0.4606\n",
            "Epoch 886/1000 | Loss: 0.4605\n",
            "Epoch 887/1000 | Loss: 0.4603\n",
            "Epoch 888/1000 | Loss: 0.4602\n",
            "Epoch 889/1000 | Loss: 0.4601\n",
            "Epoch 890/1000 | Loss: 0.4599\n",
            "Epoch 891/1000 | Loss: 0.4598\n",
            "Epoch 892/1000 | Loss: 0.4596\n",
            "Epoch 893/1000 | Loss: 0.4595\n",
            "Epoch 894/1000 | Loss: 0.4594\n",
            "Epoch 895/1000 | Loss: 0.4592\n",
            "Epoch 896/1000 | Loss: 0.4591\n",
            "Epoch 897/1000 | Loss: 0.4589\n",
            "Epoch 898/1000 | Loss: 0.4588\n",
            "Epoch 899/1000 | Loss: 0.4587\n",
            "Epoch 900/1000 | Loss: 0.4585\n",
            "Epoch 901/1000 | Loss: 0.4584\n",
            "Epoch 902/1000 | Loss: 0.4582\n",
            "Epoch 903/1000 | Loss: 0.4581\n",
            "Epoch 904/1000 | Loss: 0.4580\n",
            "Epoch 905/1000 | Loss: 0.4578\n",
            "Epoch 906/1000 | Loss: 0.4577\n",
            "Epoch 907/1000 | Loss: 0.4575\n",
            "Epoch 908/1000 | Loss: 0.4574\n",
            "Epoch 909/1000 | Loss: 0.4573\n",
            "Epoch 910/1000 | Loss: 0.4571\n",
            "Epoch 911/1000 | Loss: 0.4570\n",
            "Epoch 912/1000 | Loss: 0.4568\n",
            "Epoch 913/1000 | Loss: 0.4567\n",
            "Epoch 914/1000 | Loss: 0.4566\n",
            "Epoch 915/1000 | Loss: 0.4564\n",
            "Epoch 916/1000 | Loss: 0.4563\n",
            "Epoch 917/1000 | Loss: 0.4561\n",
            "Epoch 918/1000 | Loss: 0.4560\n",
            "Epoch 919/1000 | Loss: 0.4559\n",
            "Epoch 920/1000 | Loss: 0.4557\n",
            "Epoch 921/1000 | Loss: 0.4556\n",
            "Epoch 922/1000 | Loss: 0.4555\n",
            "Epoch 923/1000 | Loss: 0.4553\n",
            "Epoch 924/1000 | Loss: 0.4552\n",
            "Epoch 925/1000 | Loss: 0.4550\n",
            "Epoch 926/1000 | Loss: 0.4549\n",
            "Epoch 927/1000 | Loss: 0.4548\n",
            "Epoch 928/1000 | Loss: 0.4546\n",
            "Epoch 929/1000 | Loss: 0.4545\n",
            "Epoch 930/1000 | Loss: 0.4544\n",
            "Epoch 931/1000 | Loss: 0.4542\n",
            "Epoch 932/1000 | Loss: 0.4541\n",
            "Epoch 933/1000 | Loss: 0.4539\n",
            "Epoch 934/1000 | Loss: 0.4538\n",
            "Epoch 935/1000 | Loss: 0.4537\n",
            "Epoch 936/1000 | Loss: 0.4535\n",
            "Epoch 937/1000 | Loss: 0.4534\n",
            "Epoch 938/1000 | Loss: 0.4533\n",
            "Epoch 939/1000 | Loss: 0.4531\n",
            "Epoch 940/1000 | Loss: 0.4530\n",
            "Epoch 941/1000 | Loss: 0.4528\n",
            "Epoch 942/1000 | Loss: 0.4527\n",
            "Epoch 943/1000 | Loss: 0.4526\n",
            "Epoch 944/1000 | Loss: 0.4524\n",
            "Epoch 945/1000 | Loss: 0.4523\n",
            "Epoch 946/1000 | Loss: 0.4522\n",
            "Epoch 947/1000 | Loss: 0.4520\n",
            "Epoch 948/1000 | Loss: 0.4519\n",
            "Epoch 949/1000 | Loss: 0.4518\n",
            "Epoch 950/1000 | Loss: 0.4516\n",
            "Epoch 951/1000 | Loss: 0.4515\n",
            "Epoch 952/1000 | Loss: 0.4514\n",
            "Epoch 953/1000 | Loss: 0.4512\n",
            "Epoch 954/1000 | Loss: 0.4511\n",
            "Epoch 955/1000 | Loss: 0.4509\n",
            "Epoch 956/1000 | Loss: 0.4508\n",
            "Epoch 957/1000 | Loss: 0.4507\n",
            "Epoch 958/1000 | Loss: 0.4505\n",
            "Epoch 959/1000 | Loss: 0.4504\n",
            "Epoch 960/1000 | Loss: 0.4503\n",
            "Epoch 961/1000 | Loss: 0.4501\n",
            "Epoch 962/1000 | Loss: 0.4500\n",
            "Epoch 963/1000 | Loss: 0.4499\n",
            "Epoch 964/1000 | Loss: 0.4497\n",
            "Epoch 965/1000 | Loss: 0.4496\n",
            "Epoch 966/1000 | Loss: 0.4495\n",
            "Epoch 967/1000 | Loss: 0.4493\n",
            "Epoch 968/1000 | Loss: 0.4492\n",
            "Epoch 969/1000 | Loss: 0.4491\n",
            "Epoch 970/1000 | Loss: 0.4489\n",
            "Epoch 971/1000 | Loss: 0.4488\n",
            "Epoch 972/1000 | Loss: 0.4487\n",
            "Epoch 973/1000 | Loss: 0.4485\n",
            "Epoch 974/1000 | Loss: 0.4484\n",
            "Epoch 975/1000 | Loss: 0.4483\n",
            "Epoch 976/1000 | Loss: 0.4481\n",
            "Epoch 977/1000 | Loss: 0.4480\n",
            "Epoch 978/1000 | Loss: 0.4479\n",
            "Epoch 979/1000 | Loss: 0.4477\n",
            "Epoch 980/1000 | Loss: 0.4476\n",
            "Epoch 981/1000 | Loss: 0.4475\n",
            "Epoch 982/1000 | Loss: 0.4473\n",
            "Epoch 983/1000 | Loss: 0.4472\n",
            "Epoch 984/1000 | Loss: 0.4471\n",
            "Epoch 985/1000 | Loss: 0.4469\n",
            "Epoch 986/1000 | Loss: 0.4468\n",
            "Epoch 987/1000 | Loss: 0.4467\n",
            "Epoch 988/1000 | Loss: 0.4465\n",
            "Epoch 989/1000 | Loss: 0.4464\n",
            "Epoch 990/1000 | Loss: 0.4463\n",
            "Epoch 991/1000 | Loss: 0.4461\n",
            "Epoch 992/1000 | Loss: 0.4460\n",
            "Epoch 993/1000 | Loss: 0.4459\n",
            "Epoch 994/1000 | Loss: 0.4457\n",
            "Epoch 995/1000 | Loss: 0.4456\n",
            "Epoch 996/1000 | Loss: 0.4455\n",
            "Epoch 997/1000 | Loss: 0.4453\n",
            "Epoch 998/1000 | Loss: 0.4452\n",
            "Epoch 999/1000 | Loss: 0.4451\n",
            "Epoch 1000/1000 | Loss: 0.4450\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.3673 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9744 | Above 50%: True\n"
          ]
        }
      ],
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# training data + ground truth\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # 1 in + 1 out\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec4bdeb",
      "metadata": {
        "id": "eec4bdeb"
      },
      "source": [
        "## 7 diabets_logistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "902bfc58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "902bfc58",
        "outputId": "0b47cb4b-ce50-432b-bb34-77512355634b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "../data/diabetes.csv.gz not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-8d53dc63f7e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/diabetes.csv.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1381\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1382\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: ../data/diabetes.csv.gz not found."
          ]
        }
      ],
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('../data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
        "x_data = from_numpy(xy[:, 0:-1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ffdde20",
      "metadata": {
        "id": "7ffdde20"
      },
      "source": [
        "## 8\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec56d62",
      "metadata": {
        "id": "8ec56d62"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch import from_numpy\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "    def __init__(self, csv_path='/content/diabetes-2.csv'):\n",
        "        xy = np.loadtxt(\n",
        "            csv_path,\n",
        "            delimiter=',',\n",
        "            dtype=np.float32,\n",
        "            skiprows=1\n",
        "        )\n",
        "        self.x = from_numpy(xy[:, 0:-1])\n",
        "        self.y = from_numpy(xy[:, -1:])\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = tensor(inputs), tensor(labels)\n",
        "        #training process\n",
        "        print(f'Epoch: {i} | Inputs {inputs.data} | Labels {labels.data}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b102e5",
      "metadata": {
        "id": "89b102e5"
      },
      "source": [
        "## logistic\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d9be56",
      "metadata": {
        "id": "24d9be56"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, from_numpy, optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = nn.BCELoss(reduction='sum')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        y_pred = model(inputs)\n",
        "\n",
        "        loss = criterion(y_pred, labels)\n",
        "        print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c02fba",
      "metadata": {
        "id": "f2c02fba"
      },
      "source": [
        "## 9 - softmax loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413e03f1",
      "metadata": {
        "id": "413e03f1"
      },
      "outputs": [],
      "source": [
        "from torch import nn, tensor, max\n",
        "import numpy as np\n",
        "\n",
        "# Cross entropy\n",
        "# 0: 1 0 0\n",
        "# 1: 0 1 0\n",
        "# 2: 0 0 1\n",
        "Y = np.array([1, 0, 0])\n",
        "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
        "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n",
        "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')\n",
        "\n",
        "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "Y = tensor([0], requires_grad=False)\n",
        "\n",
        "Y_pred1 = tensor([[2.0, 1.0, 0.1]])\n",
        "Y_pred2 = tensor([[0.5, 2.0, 0.3]])\n",
        "\n",
        "l1 = loss(Y_pred1, Y)\n",
        "l2 = loss(Y_pred2, Y)\n",
        "\n",
        "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n",
        "print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\n",
        "print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')\n",
        "\n",
        "\n",
        "Y = tensor([2, 0, 1], requires_grad=False)\n",
        "\n",
        "Y_pred1 = tensor([[0.1, 0.2, 0.9],\n",
        "                  [1.1, 0.1, 0.2],\n",
        "                  [0.2, 2.1, 0.1]])\n",
        "\n",
        "Y_pred2 = tensor([[0.8, 0.2, 0.3],\n",
        "                  [0.2, 0.3, 0.5],\n",
        "                  [0.2, 0.2, 0.5]])\n",
        "\n",
        "l1 = loss(Y_pred1, Y)\n",
        "l2 = loss(Y_pred2, Y)\n",
        "print(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28a7faeb",
      "metadata": {
        "id": "28a7faeb"
      },
      "source": [
        "## softmax_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3662e165",
      "metadata": {
        "id": "3662e165"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "batch_size = 64\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# pipeline\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(784, 520)\n",
        "        self.l2 = nn.Linear(520, 320)\n",
        "        self.l3 = nn.Linear(320, 240)\n",
        "        self.l4 = nn.Linear(240, 120)\n",
        "        self.l5 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))\n",
        "        return self.l5(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'------------------------------------------------------\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7751ef17",
      "metadata": {
        "id": "7751ef17"
      },
      "source": [
        "## 10 cnn mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSeSz4HIqoo4"
      },
      "id": "WSeSz4HIqoo4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Sfrp0G9qomE"
      },
      "id": "5Sfrp0G9qomE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b728e313",
      "metadata": {
        "id": "b728e313"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data/',train=True, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff72228",
      "metadata": {
        "id": "dff72228"
      },
      "source": [
        "## `11 toy_inception MNIST\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a1cfa2",
      "metadata": {
        "id": "50a1cfa2"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 9\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionA, self).__init__()\n",
        "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.incept1 = InceptionA(in_channels=10)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
        "        self.incept2 = InceptionA(in_channels=20)\n",
        "\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(1408, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = self.incept1(x)\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = self.incept2(x)\n",
        "        x = x.view(in_size, -1)  # flatten\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)  # default reduction='mean'\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            loss_value = loss.item()\n",
        "            processed = batch_idx * len(data)\n",
        "            total = len(train_loader.dataset)\n",
        "            percent = 100. * batch_idx / len(train_loader)\n",
        "            print(f'Train Epoch: {epoch} [{processed}/{total} ({percent:.0f}%)]\\tLoss: {loss_value:.6f}')\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c56657",
      "metadata": {
        "id": "c8c56657"
      },
      "source": [
        "## 12 RNN basics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1252d7d5",
      "metadata": {
        "id": "1252d7d5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# one hot encoding\n",
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]\n",
        "\n",
        "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
        "\n",
        "hidden = Variable(torch.randn(1, 1, 2))\n",
        "\n",
        "# propagate input through RNN\n",
        "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
        "for one in inputs:\n",
        "    one = one.view(1, 1, -1)\n",
        "\n",
        "    out, hidden = cell(one, hidden)\n",
        "    print(\"one input size\", one.size(), \"out size\", out.size())\n",
        "\n",
        "\n",
        "inputs = inputs.view(1, 5, -1)\n",
        "out, hidden = cell(inputs, hidden)\n",
        "print(\"sequence input size\", inputs.size(), \"out size\", out.size())\n",
        "\n",
        "\n",
        "hidden = Variable(torch.randn(1, 3, 2))\n",
        "\n",
        "\n",
        "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
        "                                [e, o, l, l, l],\n",
        "                                [l, l, e, e, l]]))\n",
        "\n",
        "# B x S x I\n",
        "out, hidden = cell(inputs, hidden)\n",
        "print(\"batch input size\", inputs.size(), \"out size\", out.size())\n",
        "\n",
        "\n",
        "cell = nn.RNN(input_size=4, hidden_size=2)\n",
        "\n",
        "inputs = inputs.transpose(dim0=0, dim1=1)\n",
        "\n",
        "out, hidden = cell(inputs, hidden)\n",
        "print(\"batch input size\", inputs.size(), \"out size\", out.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fa28b2",
      "metadata": {
        "id": "b6fa28b2"
      },
      "source": [
        "## hello RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbfcdf09",
      "metadata": {
        "id": "dbfcdf09"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(777)  # reproducibility\n",
        "#            0    1    2    3    4\n",
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "\n",
        "x_data = [0, 1, 0, 2, 3, 3]   # hihell\n",
        "one_hot_lookup = [[1, 0, 0, 0, 0],\n",
        "                  [0, 1, 0, 0, 0],\n",
        "                  [0, 0, 1, 0, 0],\n",
        "                  [0, 0, 0, 1, 0],\n",
        "                  [0, 0, 0, 0, 1]]\n",
        "\n",
        "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
        "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
        "\n",
        "inputs = Variable(torch.Tensor(x_one_hot))\n",
        "labels = Variable(torch.LongTensor(y_data))\n",
        "\n",
        "num_classes = 5\n",
        "input_size = 5  # one hot size\n",
        "hidden_size = 5  # output from the RNN\n",
        "batch_size = 1   # one sentence\n",
        "sequence_length = 1\n",
        "num_layers = 1\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=input_size,\n",
        "                          hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, hidden, x):\n",
        "        #reshape\n",
        "        x = x.view(batch_size, sequence_length, input_size)\n",
        "\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        return hidden, out.view(-1, num_classes)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
        "\n",
        "\n",
        "# RNN model\n",
        "model = Model()\n",
        "print(model)\n",
        "\n",
        "# crossEntropyLoss = LogSoftmax +  NLLLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# train\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    hidden = model.init_hidden()\n",
        "\n",
        "    sys.stdout.write(\"predicted string: \")\n",
        "    for input, label in zip(inputs, labels):\n",
        "        hidden, output = model(hidden, input)\n",
        "        val, idx = output.max(1)\n",
        "        sys.stdout.write(idx2char[idx.data[0]])\n",
        "        loss += criterion(output, torch.LongTensor([label]))\n",
        "\n",
        "    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Learning finished !!! \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d9c620",
      "metadata": {
        "id": "01d9c620"
      },
      "source": [
        "## `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6f8174",
      "metadata": {
        "id": "4a6f8174"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(777)\n",
        "\n",
        "\n",
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "\n",
        "x_data = [[0, 1, 0, 2, 3, 3]]\n",
        "x_one_hot = [[[1, 0, 0, 0, 0],\n",
        "              [0, 1, 0, 0, 0],\n",
        "              [1, 0, 0, 0, 0],\n",
        "              [0, 0, 1, 0, 0],\n",
        "              [0, 0, 0, 1, 0],\n",
        "              [0, 0, 0, 1, 0]]]\n",
        "\n",
        "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
        "\n",
        "inputs = Variable(torch.Tensor(x_one_hot))\n",
        "labels = Variable(torch.LongTensor(y_data))\n",
        "\n",
        "num_classes = 5\n",
        "input_size = 5\n",
        "hidden_size = 5\n",
        "batch_size = 1\n",
        "sequence_length = 6\n",
        "num_layers = 1\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
        "\n",
        "        out, _ = self.rnn(x, h_0)\n",
        "        return out.view(-1, num_classes)\n",
        "\n",
        "\n",
        "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
        "print(rnn)\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    outputs = rnn(inputs)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, idx = outputs.max(1)\n",
        "    idx = idx.data.numpy()\n",
        "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
        "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
        "    print(\"Predicted string: \", ''.join(result_str))\n",
        "\n",
        "print(\"Learning finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db037f82",
      "metadata": {
        "id": "db037f82"
      },
      "source": [
        "## hello RNN EMBEDDINGS\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb6a488",
      "metadata": {
        "id": "fcb6a488"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(777)\n",
        "\n",
        "\n",
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "\n",
        "\n",
        "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
        "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
        "\n",
        "inputs = Variable(torch.LongTensor(x_data))\n",
        "labels = Variable(torch.LongTensor(y_data))\n",
        "\n",
        "num_classes = 5\n",
        "input_size = 5\n",
        "embedding_size = 10\n",
        "hidden_size = 5\n",
        "sequence_length = 6\n",
        "num_layers = 1\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(input_size=embedding_size,\n",
        "                          hidden_size=5, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.view(batch_size, sequence_length, -1)\n",
        "\n",
        "\n",
        "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
        "        out, _ = self.rnn(emb, h_0)\n",
        "        return self.fc(out.view(-1, num_classes))\n",
        "\n",
        "\n",
        "model = Model(num_layers, hidden_size)\n",
        "print(model)\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    outputs = model(inputs)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, idx = outputs.max(1)\n",
        "    idx = idx.data.numpy()\n",
        "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
        "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
        "    print(\"Predicted string: \", ''.join(result_str))\n",
        "\n",
        "print(\"Learning finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FCO5z20J4EUo"
      },
      "id": "FCO5z20J4EUo"
    },
    {
      "cell_type": "markdown",
      "id": "26fe037b",
      "metadata": {
        "id": "26fe037b"
      },
      "source": [
        "## rnn_classification basics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/name_dataset.py\" /content/\n"
      ],
      "metadata": {
        "id": "oHTjpuD_BUwu"
      },
      "id": "oHTjpuD_BUwu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e777bda7",
      "metadata": {
        "id": "e777bda7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from name_dataset import NameDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "HIDDEN_SIZE = 100\n",
        "N_CHARS = 128\n",
        "N_CLASSES = 18\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        batch_size = input.size(0)\n",
        "\n",
        "        input = input.t()\n",
        "\n",
        "        print(\"  input\", input.size())\n",
        "        embedded = self.embedding(input)\n",
        "        print(\"  embedding\", embedded.size())\n",
        "\n",
        "        hidden = self._init_hidden(batch_size)\n",
        "\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        print(\"  gru hidden output\", hidden.size())\n",
        "\n",
        "        fc_output = self.fc(hidden)\n",
        "        print(\"  fc output\", fc_output.size())\n",
        "        return fc_output\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "        return Variable(hidden)\n",
        "\n",
        "\n",
        "def str2ascii_arr(msg):\n",
        "    arr = [ord(c) for c in msg]\n",
        "    return arr, len(arr)\n",
        "\n",
        "def pad_sequences(vectorized_seqs, seq_lengths):\n",
        "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "    return seq_tensor\n",
        "\n",
        "def make_variables(names):\n",
        "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
        "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
        "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
        "    return pad_sequences(vectorized_seqs, seq_lengths)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    names = ['adylov', 'solan', 'hard', 'san']\n",
        "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n",
        "\n",
        "    for name in names:\n",
        "        arr, _ = str2ascii_arr(name)\n",
        "        inp = Variable(torch.LongTensor([arr]))\n",
        "        out = classifier(inp)\n",
        "        print(\"in\", inp.size(), \"out\", out.size())\n",
        "\n",
        "\n",
        "    inputs = make_variables(names)\n",
        "    out = classifier(inputs)\n",
        "    print(\"batch in\", inputs.size(), \"batch out\", out.size())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991ae154",
      "metadata": {
        "id": "991ae154"
      },
      "source": [
        "## rnn_classification\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!mkdir -p /content/data\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/names_train.csv.gz\" /content/data/\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/names_test.csv.gz\"  /content/data/\n",
        "\n",
        "!ls -l /content/data\n"
      ],
      "metadata": {
        "id": "BjoXagv6-1tG"
      },
      "id": "BjoXagv6-1tG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ccb6e2",
      "metadata": {
        "id": "82ccb6e2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from name_dataset import NameDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "HIDDEN_SIZE = 100\n",
        "N_LAYERS = 2\n",
        "BATCH_SIZE = 256\n",
        "N_EPOCHS = 100\n",
        "\n",
        "test_dataset = NameDataset(is_train_set=False)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "train_dataset = NameDataset(is_train_set=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "N_COUNTRIES = len(train_dataset.get_countries())\n",
        "print(N_COUNTRIES, \"countries\")\n",
        "N_CHARS = 128\n",
        "\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def create_variable(tensor):\n",
        "    if torch.cuda.is_available():\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
        "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "\n",
        "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "    seq_tensor = seq_tensor[perm_idx]\n",
        "\n",
        "    target = countries2tensor(countries)\n",
        "    if len(countries):\n",
        "        target = target[perm_idx]\n",
        "\n",
        "\n",
        "    return create_variable(seq_tensor), \\\n",
        "        create_variable(seq_lengths), \\\n",
        "        create_variable(target)\n",
        "\n",
        "\n",
        "def make_variables(names, countries):\n",
        "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
        "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
        "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
        "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
        "\n",
        "\n",
        "def str2ascii_arr(msg):\n",
        "    arr = [ord(c) for c in msg]\n",
        "    return arr, len(arr)\n",
        "\n",
        "\n",
        "def countries2tensor(countries):\n",
        "    country_ids = [train_dataset.get_country_id(\n",
        "        country) for country in countries]\n",
        "    return torch.LongTensor(country_ids)\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    # MODEL\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_directions = int(bidirectional) + 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, seq_lengths):\n",
        "\n",
        "        input = input.t()\n",
        "        batch_size = input.size(1)\n",
        "\n",
        "        hidden = self._init_hidden(batch_size)\n",
        "\n",
        "        #  S x B -> S x B x I\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        gru_input = pack_padded_sequence(\n",
        "            embedded, seq_lengths.data.cpu().numpy())\n",
        "\n",
        "        self.gru.flatten_parameters()\n",
        "        output, hidden = self.gru(gru_input, hidden)\n",
        "\n",
        "\n",
        "        fc_output = self.fc(hidden[-1])\n",
        "        return fc_output\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
        "                             batch_size, self.hidden_size)\n",
        "        return create_variable(hidden)\n",
        "\n",
        "\n",
        "# Train cycle\n",
        "def train():\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (names, countries) in enumerate(train_loader, 1):\n",
        "        input, seq_lengths, target = make_variables(names, countries)\n",
        "        output = classifier(input, seq_lengths)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        classifier.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
        "                time_since(start), epoch,  i *\n",
        "                len(names), len(train_loader.dataset),\n",
        "                100. * i * len(names) / len(train_loader.dataset),\n",
        "                total_loss / i * len(names)))\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def test(name=None):\n",
        "    if name:\n",
        "        input, seq_lengths, target = make_variables([name], [])\n",
        "        output = classifier(input, seq_lengths)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        country_id = pred.cpu().numpy()[0][0]\n",
        "        print(name, \"is\", train_dataset.get_country(country_id))\n",
        "        return\n",
        "\n",
        "    print(\"evaluating trained model ...\")\n",
        "    correct = 0\n",
        "    train_data_size = len(test_loader.dataset)\n",
        "\n",
        "    for names, countries in test_loader:\n",
        "        input, seq_lengths, target = make_variables(names, countries)\n",
        "        output = classifier(input, seq_lengths)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, train_data_size, 100. * correct / train_data_size))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        # dim = 0     [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n",
        "        classifier = nn.DataParallel(classifier)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        classifier.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start = time.time()\n",
        "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        train()\n",
        "\n",
        "        test()\n",
        "\n",
        "        test(\"Sung\")\n",
        "        test(\"Jungwoo\")\n",
        "        test(\"Soojin\")\n",
        "        test(\"Nako\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe014b89",
      "metadata": {
        "id": "fe014b89"
      },
      "source": [
        "## RNN CHAR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!mkdir -p /content/data\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/shakespeare.txt.gz\" /content/data/\n",
        "\n",
        "!ls -l /content/data\n"
      ],
      "metadata": {
        "id": "Xoz9GckmH8mc"
      },
      "id": "Xoz9GckmH8mc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bcd036d",
      "metadata": {
        "id": "1bcd036d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from text_loader import TextDataset\n",
        "\n",
        "hidden_size   = 100\n",
        "n_layers      = 3\n",
        "batch_size    = 1\n",
        "n_epochs      = 100\n",
        "n_characters  = 128\n",
        "device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers    = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru       = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.linear    = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_char, hidden):\n",
        "        # LongTensor scalar\n",
        "        embed  = self.embedding(input_char.view(1, -1))\n",
        "        embed  = embed.view(1, 1, -1)\n",
        "        output, hidden = self.gru(embed, hidden)\n",
        "        output = self.linear(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        h = torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
        "        return h\n",
        "\n",
        "\n",
        "def str2tensor(string):\n",
        "    arr = [ord(c) for c in string]\n",
        "    t   = torch.LongTensor(arr).to(device)\n",
        "    return t\n",
        "\n",
        "\n",
        "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
        "    decoder.eval()\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = str2tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "\n",
        "    inp = prime_input[-1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(predict_len):\n",
        "            output, hidden = decoder(inp, hidden)\n",
        "\n",
        "            dist = output.view(-1).div(temperature).exp()\n",
        "            top_i = torch.multinomial(dist, 1).item()\n",
        "            predicted_char = chr(top_i)\n",
        "            predicted += predicted_char\n",
        "            inp = torch.LongTensor([top_i]).to(device)\n",
        "\n",
        "    return predicted\n",
        "\n",
        "\n",
        "def train_teacher_forcing(line):\n",
        "    decoder.train()\n",
        "    input_t  = str2tensor(line[:-1])\n",
        "    target_t = str2tensor(line[1:])\n",
        "\n",
        "    hidden = decoder.init_hidden()\n",
        "    loss = 0.0\n",
        "    for c in range(len(input_t)):\n",
        "        output, hidden = decoder(input_t[c], hidden)\n",
        "        loss += criterion(output, target_t[c].unsqueeze(0))\n",
        "    decoder.zero_grad()\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / len(input_t)\n",
        "\n",
        "\n",
        "def train_sampling(line):\n",
        "    decoder.train()\n",
        "    input_t  = str2tensor(line[:-1])\n",
        "    target_t = str2tensor(line[1:])\n",
        "\n",
        "    hidden   = decoder.init_hidden()\n",
        "    decoder_in = input_t[0]\n",
        "    loss = 0.0\n",
        "    for c in range(len(input_t)):\n",
        "        output, hidden = decoder(decoder_in, hidden)\n",
        "        loss += criterion(output, target_t[c].unsqueeze(0))\n",
        "        decoder_in = output.argmax(dim=1)\n",
        "    decoder.zero_grad()\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / len(input_t)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    decoder = RNN(n_characters, hidden_size, n_characters, n_layers).to(device)\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=TextDataset(),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    print(f\"Training for {n_epochs} epochs on {device}\")\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        for i, (lines, _) in enumerate(train_loader):\n",
        "            loss = train_sampling(lines[0])\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                pct = epoch / n_epochs * 100\n",
        "                print(f'[(Epoch {epoch} {pct:.1f}% ) loss: {loss:.4f}]')\n",
        "                print(generate(decoder, 'Wh', 100), '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTUw79ZA3Z5F"
      },
      "id": "bTUw79ZA3Z5F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c737db69",
      "metadata": {
        "id": "c737db69"
      },
      "source": [
        "## pack_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64ca939",
      "metadata": {
        "id": "e64ca939"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "\n",
        "def flatten(l):\n",
        "    return list(itertools.chain.from_iterable(l))\n",
        "\n",
        "seqs = ['ghatmasala', 'nicela', 'chutpakodas']\n",
        "\n",
        "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
        "\n",
        "# model\n",
        "embedding_size = 3\n",
        "embed = nn.Embedding(len(vocab), embedding_size)\n",
        "lstm = nn.LSTM(embedding_size, 5)\n",
        "\n",
        "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
        "print(\"vectorized_seqs\", vectorized_seqs)\n",
        "\n",
        "print([x for x in map(len, vectorized_seqs)])\n",
        "# get the length of each seq in your batch\n",
        "seq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])\n",
        "\n",
        "\n",
        "seq_tensor = Variable(torch.zeros(\n",
        "    (len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "print(\"seq_tensor\", seq_tensor)\n",
        "\n",
        "#sort by length\n",
        "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "seq_tensor = seq_tensor[perm_idx]\n",
        "\n",
        "print(\"seq_tensor after sorting\", seq_tensor)\n",
        "\n",
        "seq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
        "print(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n",
        "\n",
        "\n",
        "embeded_seq_tensor = embed(seq_tensor)\n",
        "print(\"seq_tensor after embeding\", embeded_seq_tensor.size(), seq_tensor.data)\n",
        "\n",
        "packed_input = pack_padded_sequence(\n",
        "    embeded_seq_tensor, seq_lengths.cpu().numpy())\n",
        "\n",
        "# throw them through your LSTM\n",
        "packed_output, (ht, ct) = lstm(packed_input)\n",
        "\n",
        "# unpack your output\n",
        "output, _ = pad_packed_sequence(packed_output)\n",
        "print(\"Lstm output\", output.size(), output.data)\n",
        "\n",
        "# if you just want the final hidden state?\n",
        "print(\"Last output\", ht[-1].size(), ht[-1].data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b54cb8",
      "metadata": {
        "id": "f0b54cb8"
      },
      "source": [
        "## seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a99936",
      "metadata": {
        "id": "48a99936"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from text_loader import TextDataset\n",
        "import seq2seq_models as sm\n",
        "from seq2seq_models import str2tensor, EOS_token, SOS_token\n",
        "\n",
        "HIDDEN_SIZE = 100\n",
        "N_LAYERS = 1\n",
        "BATCH_SIZE = 1\n",
        "N_EPOCH = 100\n",
        "N_CHARS = 128  # ASCII\n",
        "\n",
        "\n",
        "def test():\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    word_input = str2tensor('hello')\n",
        "    encoder_outputs, encoder_hidden = encoder(word_input, encoder_hidden)\n",
        "    print(encoder_outputs)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    word_target = str2tensor('pytorch')\n",
        "    for c in range(len(word_target)):\n",
        "        decoder_output, decoder_hidden = decoder(\n",
        "            word_target[c], decoder_hidden)\n",
        "        print(decoder_output.size(), decoder_hidden.size())\n",
        "\n",
        "\n",
        "def train(src, target):\n",
        "    src_var = str2tensor(src)\n",
        "    target_var = str2tensor(target, eos=True)\n",
        "\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n",
        "\n",
        "    hidden = encoder_hidden\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(len(target_var)):\n",
        "\n",
        "        token = target_var[c - 1] if c else str2tensor(SOS_token)\n",
        "        output, hidden = decoder(token, hidden)\n",
        "        loss += criterion(output, target_var[c])\n",
        "\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.data[0] / len(target_var)\n",
        "\n",
        "\n",
        "\n",
        "def translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n",
        "    input_var = str2tensor(enc_input)\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
        "\n",
        "    hidden = encoder_hidden\n",
        "\n",
        "    predicted = ''\n",
        "    dec_input = str2tensor(SOS_token)\n",
        "    for c in range(predict_len):\n",
        "        output, hidden = decoder(dec_input, hidden)\n",
        "\n",
        "\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        if top_i is EOS_token:\n",
        "            break\n",
        "\n",
        "        predicted_char = chr(top_i)\n",
        "        predicted += predicted_char\n",
        "\n",
        "        dec_input = str2tensor(predicted_char)\n",
        "\n",
        "    return enc_input, predicted\n",
        "\n",
        "\n",
        "encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n",
        "decoder = sm.DecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    decoder.cuda()\n",
        "    encoder.cuda()\n",
        "print(encoder, decoder)\n",
        "test()\n",
        "\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=TextDataset(),\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "print(\"Training for %d epochs...\" % N_EPOCH)\n",
        "for epoch in range(1, N_EPOCH + 1):\n",
        "    for i, (srcs, targets) in enumerate(train_loader):\n",
        "        train_loss = train(srcs[0], targets[0])  # batch= 1\n",
        "\n",
        "        if i % 100 is 0:\n",
        "            print('[(%d %d%%) %.4f]' %\n",
        "                  (epoch, epoch / N_EPOCH * 100, train_loss))\n",
        "            print(translate(srcs[0]), '\\n')\n",
        "            print(translate(), '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f89b03",
      "metadata": {
        "id": "a3f89b03"
      },
      "source": [
        "## seq2seq\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229fd462",
      "metadata": {
        "id": "229fd462"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from text_loader import TextDataset\n",
        "import seq2seq_models as sm\n",
        "from seq2seq_models import cuda_variable, str2tensor, EOS_token, SOS_token\n",
        "\n",
        "\n",
        "N_LAYERS = 1\n",
        "BATCH_SIZE = 1\n",
        "N_EPOCH = 100\n",
        "N_CHARS = 128  #ASCII\n",
        "HIDDEN_SIZE = N_CHARS\n",
        "\n",
        "\n",
        "def test():\n",
        "    encoder_test = sm.EncoderRNN(10, 10, 2)\n",
        "    decoder_test = sm.AttnDecoderRNN(10, 10, 2)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        encoder_test.cuda()\n",
        "        decoder_test.cuda()\n",
        "\n",
        "    encoder_hidden = encoder_test.init_hidden()\n",
        "    word_input = cuda_variable(torch.LongTensor([1, 2, 3]))\n",
        "    encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
        "    print(encoder_outputs.size())\n",
        "\n",
        "    word_target = cuda_variable(torch.LongTensor([1, 2, 3]))\n",
        "    decoder_attns = torch.zeros(1, 3, 3)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    for c in range(len(word_target)):\n",
        "        decoder_output, decoder_hidden, decoder_attn = \\\n",
        "            decoder_test(word_target[c],\n",
        "                         decoder_hidden, encoder_outputs)\n",
        "        print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n",
        "        decoder_attns[0, c] = decoder_attn.squeeze(0).cpu().data\n",
        "\n",
        "\n",
        "def train(src, target):\n",
        "    loss = 0\n",
        "\n",
        "    src_var = str2tensor(src)\n",
        "    target_var = str2tensor(target, eos=True)  #EOS token\n",
        "\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n",
        "\n",
        "    hidden = encoder_hidden\n",
        "\n",
        "    for c in range(len(target_var)):\n",
        "    # sos first\n",
        "        token = target_var[c - 1] if c else str2tensor(SOS_token)\n",
        "        output, hidden, attention = decoder(token, hidden, encoder_outputs)\n",
        "        loss += criterion(output, target_var[c])\n",
        "\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.data[0] / len(target_var)\n",
        "\n",
        "\n",
        "def translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n",
        "    input_var = str2tensor(enc_input)\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
        "\n",
        "    hidden = encoder_hidden\n",
        "\n",
        "    predicted = ''\n",
        "    dec_input = str2tensor(SOS_token)\n",
        "    attentions = []\n",
        "    for c in range(predict_len):\n",
        "        output, hidden, attention = decoder(dec_input, hidden, encoder_outputs)\n",
        "        # sample from the network as a multinominal distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        attentions.append(attention.view(-1).data.cpu().numpy().tolist())\n",
        "\n",
        "        # stop st EOS\n",
        "        if top_i is EOS_token:\n",
        "            break\n",
        "\n",
        "        predicted_char = chr(top_i)\n",
        "        predicted += predicted_char\n",
        "\n",
        "        dec_input = str2tensor(predicted_char)\n",
        "\n",
        "    return predicted, attentions\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    encoder = sm.EncoderRNN(N_CHARS, HIDDEN_SIZE, N_LAYERS)\n",
        "    decoder = sm.AttnDecoderRNN(HIDDEN_SIZE, N_CHARS, N_LAYERS)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        decoder.cuda()\n",
        "        encoder.cuda()\n",
        "    print(encoder, decoder)\n",
        "\n",
        "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "    optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(dataset=TextDataset(),\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=2)\n",
        "\n",
        "    print(\"Training for %d epochs...\" % N_EPOCH)\n",
        "    for epoch in range(1, N_EPOCH + 1):\n",
        "        #srcs + targets from data loader\n",
        "        for i, (srcs, targets) in enumerate(train_loader):\n",
        "            train_loss = train(srcs[0], targets[0])\n",
        "\n",
        "            if i % 1000 is 0:\n",
        "                print('[(%d/%d %d%%) %.4f]' %\n",
        "                      (epoch, N_EPOCH, i * len(srcs) * 100 / len(train_loader), train_loss))\n",
        "                output, _ = translate(srcs[0])\n",
        "                print(srcs[0], output, '\\n')\n",
        "\n",
        "                output, attentions = translate()\n",
        "                print('thisissungkim.iloveyou.', output, '\\n')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}