{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4412411,"sourceType":"datasetVersion","datasetId":2585335}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"_uuid":"2585d151-c696-4186-9185-a83f7e1996b9","_cell_guid":"3c207dc0-3d85-43ac-a1d5-dd7bfb9e368d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-03T10:16:31.182331Z","iopub.execute_input":"2025-06-03T10:16:31.182569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setări directoare\ntrain_dir = '/kaggle/input/car-vs-bike-classification-dataset/Car-Bike-Dataset'\ntest_dir = train_dir  # dacă vrei să folosești tot același set pentru test, altfel setează separat","metadata":{"_uuid":"2ac38fcd-5df7-4389-9efd-564ac5df18d0","_cell_guid":"9d748b9d-aba0-4d4b-9abb-63b19e639988","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vizualizare imagini random din set\ndef show_random_images(base_dir, num_images=5):\n    image_paths = []\n    for class_name in os.listdir(base_dir):\n        class_dir = os.path.join(base_dir, class_name)\n        if os.path.isdir(class_dir):\n            for fname in os.listdir(class_dir):\n                if fname.lower().endswith(('jpg', 'jpeg', 'png')):\n                    image_paths.append(os.path.join(class_dir, fname))\n    sample_files = random.sample(image_paths, num_images)\n    plt.figure(figsize=(15, 5))\n    for idx, f in enumerate(sample_files):\n        img = Image.open(f)\n        class_name = os.path.basename(os.path.dirname(f))\n        plt.subplot(1, num_images, idx + 1)\n        plt.imshow(img)\n        plt.title(class_name)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nshow_random_images(train_dir)","metadata":{"_uuid":"3e06a89f-bc4e-42f6-b5cc-3b4cd66b9208","_cell_guid":"d4565575-4450-453f-a79f-9764e45cbcd4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split subfolders Car/Bike\nfile_paths = []\nlabels = []\nfor class_name in os.listdir(train_dir):\n    class_dir = os.path.join(train_dir, class_name)\n    if not os.path.isdir(class_dir):\n        continue\n    for fname in os.listdir(class_dir):\n        if fname.lower().endswith(('jpg', 'jpeg', 'png')):\n            file_paths.append(os.path.join(class_dir, fname))\n            labels.append(0 if class_name.lower() == 'bike' else 1)  # 0: bike, 1: car\n\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    file_paths, labels, test_size=0.2, random_state=42, stratify=labels\n)","metadata":{"_uuid":"cc7cccc1-6e2b-43e0-a6be-9909e8a5cc87","_cell_guid":"9ad6f3c3-1ec5-4221-ab33-1675634c92be","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CarBikeDataset(Dataset):\n    def __init__(self, file_paths, labels=None, transform=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n    def __len__(self):\n        return len(self.file_paths)\n    def __getitem__(self, idx):\n        img_path = self.file_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        if self.labels is not None:\n            label = self.labels[idx]\n            return image, label\n        else:\n            return image, os.path.basename(img_path)","metadata":{"_uuid":"a6207ac5-21b3-42d6-8568-308b9565493e","_cell_guid":"96a1e902-4337-4d41-bee0-d993824293fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_dataset = CarBikeDataset(train_paths, train_labels, transform)\nval_dataset = CarBikeDataset(val_paths, val_labels, transform)\n# test_set (same director) // aici pot inlocui cu folder de test \ntest_file_paths = []\nfor root, _, files in os.walk(test_dir):\n    for f in files:\n        if f.lower().endswith(('jpg', 'jpeg', 'png')):\n            test_file_paths.append(os.path.join(root, f))\ntest_dataset = CarBikeDataset(test_file_paths, labels=None, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)","metadata":{"_uuid":"77d9e001-2961-4842-be53-25960a3298a3","_cell_guid":"85cd0815-cbca-4841-b5c2-cd4d982a4c13","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"aici creez un model cu 3 straturi convolutionale +pooling pt extragerea de features \nflatten -> 3D to vector \nLinear(128 * 28 * 28, 512)\" este un strat foarte mare","metadata":{"_uuid":"87ca5a4b-6313-4918-afed-2b8d4e91020c","_cell_guid":"685d0f04-1217-4ea1-ad02-49f55898ba87","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class CarBikeCNN(nn.Module):\n    def __init__(self):\n        super(CarBikeCNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 28 * 28, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            # last layer - 2 neuroni pentru clasificare binara \n            nn.Linear(512, 2)  # car / bike\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x","metadata":{"_uuid":"d0fd103b-c95d-49e6-8f93-ce6110b82c42","_cell_guid":"08763029-b60b-4de7-b6cd-b9cefcd38938","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CarBikeCNN().to(device)","metadata":{"_uuid":"6d6b1f41-5632-4034-b6f6-d812d9e89c40","_cell_guid":"f5c2491c-324f-46ba-8018-d7998a84bd95","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Model has {count_parameters(model):,} number of trainable parameters.\")","metadata":{"_uuid":"6e88f7db-9171-42af-81bb-3d627b474a84","_cell_guid":"39b4a44c-4890-4a8f-a758-986622484e45","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nepochs = 5\ntrain_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []","metadata":{"_uuid":"f6eaa5ea-5be4-44cc-ba48-a5260447bcc2","_cell_guid":"b8def622-be47-4b70-a6ee-8c695a605339","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n    for epoch in range(epochs):\n        model.train()\n        running_loss, correct = 0.0, 0\n        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n        for images, labels in train_loop:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * images.size(0)\n            correct += (outputs.argmax(1) == labels).sum().item()\n            train_loop.set_postfix(loss=loss.item())\n        train_loss = running_loss / len(train_loader.dataset)\n        train_acc = correct / len(train_loader.dataset)\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n        val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n        with torch.no_grad():\n            for images, labels in val_loop:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * images.size(0)\n                val_correct += (outputs.argmax(1) == labels).sum().item()\n                val_loop.set_postfix(val_loss=loss.item())\n        val_loss /= len(val_loader.dataset)\n        val_acc = val_correct / len(val_loader.dataset)\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_accuracies.append(train_acc)\n        val_accuracies.append(val_acc)\n        print(f\"Epoch {epoch+1}/{epochs} => Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n    return train_losses, val_losses, train_accuracies, val_accuracies","metadata":{"_uuid":"d8ab9b66-44a8-41b0-aa53-b724517dfaba","_cell_guid":"d049a08b-4802-43b4-82bc-6131a08c1e9d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n    model, train_loader, val_loader, criterion, optimizer, device, epochs=5\n)","metadata":{"_uuid":"97d67ca0-4ea9-4803-b52e-1bc0d703bfbc","_cell_guid":"3de81fe2-524f-415e-a725-81958a52c8c8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Val Loss')\nplt.legend()\nplt.title(\"Loss\")\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Train Acc')\nplt.plot(val_accuracies, label='Val Acc')\nplt.legend()\nplt.title(\"Accuracy\")\nplt.show()","metadata":{"_uuid":"7ba1b0e1-9637-4c35-94ca-e21a18baf214","_cell_guid":"416209d0-fb1d-4b8b-ac19-d915540063a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_top_misclassified(model, dataloader, class_names=['bike', 'car'], device='cuda', top_n=5):\n    model.eval()\n    misclassified = []\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n            mis_idx = preds != labels\n            for i in range(images.size(0)):\n                if mis_idx[i]:\n                    misclassified.append((images[i].cpu(), preds[i].item(), labels[i].item()))\n    misclassified_bike = [x for x in misclassified if x[2] == 0][:top_n]\n    misclassified_car = [x for x in misclassified if x[2] == 1][:top_n]\n    def show_images(examples, title):\n        fig, axes = plt.subplots(1, len(examples), figsize=(4 * len(examples), 4))\n        fig.suptitle(title, fontsize=16)\n        if len(examples) == 1: axes = [axes]\n        for ax, (img_tensor, pred, true) in zip(axes, examples):\n            img = img_tensor.permute(1, 2, 0).numpy()\n            img = (img * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n            img = np.clip(img, 0, 1)\n            ax.imshow(img)\n            ax.set_title(f'Pred: {class_names[pred]}\\nTrue: {class_names[true]}', fontsize=12)\n            ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n    show_images(misclassified_bike, 'Top Misclassified Bike')\n    show_images(misclassified_car, 'Top Misclassified Car')","metadata":{"_uuid":"81879383-4c2b-48db-8efa-f66dab373119","_cell_guid":"64c93315-184b-4292-9760-b33be69eaeee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['bike', 'car']\nplot_top_misclassified(model, val_loader, class_names, device)","metadata":{"_uuid":"a217b2dc-f45c-42f0-a516-6a868b6be73a","_cell_guid":"3cddd6c2-6ad6-43af-b759-2ecc584d079f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/cnn_car_bike_model_weights.pth')\ntorch.save(model, '/kaggle/working/cnn_car_bike_full_model.pth')","metadata":{"_uuid":"c360610b-fb51-45af-8210-5b910de037d9","_cell_guid":"e83296b5-48bd-4d55-b7c5-5bf92ab04568","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}